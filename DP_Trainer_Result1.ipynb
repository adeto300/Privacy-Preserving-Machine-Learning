{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4952d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\adeto\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_privacy\\privacy\\optimizers\\dp_optimizer.py:267: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adeto\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\adeto\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Resizing, Rescaling\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow_addons.layers import GroupNormalization, InstanceNormalization\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasAdamOptimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06c5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install clyent==1.2.1 pyyaml==6.0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f30f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd9519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-privacy==0.5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b9958f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR10 dataset: 60k 32x32 RGB images (50k training set + 10k test set)\n",
    "(train_data, train_labels), (test_data, test_labels) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd7806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values\n",
    "#train_data, test_data = train_data / 255.0, test_data / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa5656ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the pixel values of the train and test data\n",
    "train_data = train_data.astype('float32') / 255.0\n",
    "test_data = test_data.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ba7bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ResNet50 expects input size of (224, 224, 3)\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "    Resizing(224, 224),\n",
    "    Rescaling(1./255)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882a2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "112f26fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#10 lAYERS\n",
    "# This acheives higher accuracy\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "#Next high accuracy\n",
    "models = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),  # Layer 1: Conv2D\n",
    "    LayerNormalization(),  # Layer 2: LayerNormalization\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),  # Layer 3: MaxPooling2D\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),  # Layer 4: Conv2D\n",
    "    tf.keras.layers.Dropout(0.25),  # Layer 5: Dropout\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),  # Layer 6: Conv2D\n",
    "    GroupNormalization(),  # Layer 7: GroupNormalization\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),  # Layer 8: MaxPooling2D\n",
    "    tf.keras.layers.Flatten(),  # Layer 9: Flatten\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Layer 10: Dense\n",
    "    tf.keras.layers.Dense(10, activation='softmax')  # Output layer: Dense with softmax for multi-class classification\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45ce4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ResNet50 model\n",
    "# Resizing and Rescaling layers (considered as a part of preprocessing)\n",
    "input_tensor = Input(shape=(32, 32, 3))\n",
    "x = resize_and_rescale(input_tensor)\n",
    "\n",
    "#ResNet50 has 50 layers\n",
    "base_model = ResNet50(weights=None, include_top=False, input_tensor=x)\n",
    "\n",
    "#Flattens the base model\n",
    "x = Flatten()(base_model.output)\n",
    "output_layer = Dense(10, activation='softmax')(x)  # CIFAR10 has 10 classes\n",
    "\n",
    "#model = Model(inputs=input_tensor, outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0235c1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 16, 16, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               2097408   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2340810 (8.93 MB)\n",
      "Trainable params: 2340810 (8.93 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model summary to verify the number of layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb68a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for DP-SGD\n",
    "noise_multiplier = 2.5  # Adjusted for tighter privacy guarantee\n",
    "l2_norm_clip = 1.0\n",
    "batch_size = 300 \n",
    "learning_rate = 0.001\n",
    "epochs = 400 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d691630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DP - Adam Oprimizer\n",
    "'''\n",
    "optimizer = DPKerasAdamOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=batch_size,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "# Create DP-SGD optimizer model\n",
    "optimizers = DPKerasSGDOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=batch_size,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "#l2_norm_clip: This parameter sets a threshold for clipping the L2 norm of gradients, which prevents any single \n",
    "#data point from having a disproportionate impact on the computation of gradients, thereby safeguarding individual \n",
    "#data point privacy.\n",
    "\n",
    "#noise_multiplier: This determines the amount of random noise added to the gradients during training. The noise helps \n",
    "#mask the contribution of individual data points, which is central to achieving differential privacy.\n",
    "\n",
    "#num_microbatches: This parameter controls the subdivision of a batch into smaller units, or microbatches. Processing \n",
    "#these microbatches separately and then aggregating their gradients ensures that the influence of any single data point \n",
    "#is limited.\n",
    "\n",
    "#learning_rate: While not directly related to privacy, the learning rate can impact the convergence of the training \n",
    "#process, especially when combined with the other DP parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92dab21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "#loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d56fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RMSE as a custom metric\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "678fc316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model parameters\n",
    "model.compile(optimizer=optimizers, loss=loss, metrics=['accuracy', rmse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee724d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "167/167 [==============================] - 147s 874ms/step - loss: 2.3034 - accuracy: 0.1020 - rmse: 5.2537 - val_loss: 2.2983 - val_accuracy: 0.1137 - val_rmse: 5.2471\n",
      "Epoch 2/400\n",
      "167/167 [==============================] - 169s 1s/step - loss: 2.2968 - accuracy: 0.1162 - rmse: 5.2530 - val_loss: 2.2937 - val_accuracy: 0.1349 - val_rmse: 5.2471\n",
      "Epoch 3/400\n",
      "167/167 [==============================] - 168s 1s/step - loss: 2.2930 - accuracy: 0.1287 - rmse: 5.2524 - val_loss: 2.2895 - val_accuracy: 0.1658 - val_rmse: 5.2471\n",
      "Epoch 4/400\n",
      "167/167 [==============================] - 187s 1s/step - loss: 2.2894 - accuracy: 0.1367 - rmse: 5.2525 - val_loss: 2.2850 - val_accuracy: 0.1840 - val_rmse: 5.2471\n",
      "Epoch 5/400\n",
      "167/167 [==============================] - 190s 1s/step - loss: 2.2848 - accuracy: 0.1490 - rmse: 5.2518 - val_loss: 2.2800 - val_accuracy: 0.1962 - val_rmse: 5.2471\n",
      "Epoch 6/400\n",
      "167/167 [==============================] - 189s 1s/step - loss: 2.2793 - accuracy: 0.1571 - rmse: 5.2529 - val_loss: 2.2738 - val_accuracy: 0.2035 - val_rmse: 5.2471\n",
      "Epoch 7/400\n",
      "167/167 [==============================] - 173s 1s/step - loss: 2.2725 - accuracy: 0.1681 - rmse: 5.2524 - val_loss: 2.2661 - val_accuracy: 0.2103 - val_rmse: 5.2471\n",
      "Epoch 8/400\n",
      "167/167 [==============================] - 165s 990ms/step - loss: 2.2646 - accuracy: 0.1748 - rmse: 5.2529 - val_loss: 2.2564 - val_accuracy: 0.2198 - val_rmse: 5.2471\n",
      "Epoch 9/400\n",
      "167/167 [==============================] - 199s 1s/step - loss: 2.2537 - accuracy: 0.1863 - rmse: 5.2532 - val_loss: 2.2434 - val_accuracy: 0.2278 - val_rmse: 5.2471\n",
      "Epoch 10/400\n",
      "167/167 [==============================] - 178s 1s/step - loss: 2.2413 - accuracy: 0.1911 - rmse: 5.2518 - val_loss: 2.2262 - val_accuracy: 0.2414 - val_rmse: 5.2471\n",
      "Epoch 11/400\n",
      "167/167 [==============================] - 171s 1s/step - loss: 2.2224 - accuracy: 0.2015 - rmse: 5.2523 - val_loss: 2.2027 - val_accuracy: 0.2518 - val_rmse: 5.2471\n",
      "Epoch 12/400\n",
      "167/167 [==============================] - 189s 1s/step - loss: 2.1987 - accuracy: 0.2152 - rmse: 5.2531 - val_loss: 2.1726 - val_accuracy: 0.2653 - val_rmse: 5.2471\n",
      "Epoch 13/400\n",
      "167/167 [==============================] - 179s 1s/step - loss: 2.1699 - accuracy: 0.2227 - rmse: 5.2524 - val_loss: 2.1353 - val_accuracy: 0.2725 - val_rmse: 5.2471\n",
      "Epoch 14/400\n",
      "167/167 [==============================] - 183s 1s/step - loss: 2.1348 - accuracy: 0.2308 - rmse: 5.2520 - val_loss: 2.0952 - val_accuracy: 0.2824 - val_rmse: 5.2472\n",
      "Epoch 15/400\n",
      "167/167 [==============================] - 172s 1s/step - loss: 2.1012 - accuracy: 0.2382 - rmse: 5.2526 - val_loss: 2.0573 - val_accuracy: 0.2851 - val_rmse: 5.2472\n",
      "Epoch 16/400\n",
      "167/167 [==============================] - 185s 1s/step - loss: 2.0751 - accuracy: 0.2471 - rmse: 5.2528 - val_loss: 2.0278 - val_accuracy: 0.2925 - val_rmse: 5.2473\n",
      "Epoch 17/400\n",
      "167/167 [==============================] - 175s 1s/step - loss: 2.0530 - accuracy: 0.2539 - rmse: 5.2530 - val_loss: 2.0046 - val_accuracy: 0.2940 - val_rmse: 5.2474\n",
      "Epoch 18/400\n",
      "167/167 [==============================] - 182s 1s/step - loss: 2.0347 - accuracy: 0.2600 - rmse: 5.2531 - val_loss: 1.9860 - val_accuracy: 0.3007 - val_rmse: 5.2474\n",
      "Epoch 19/400\n",
      "167/167 [==============================] - 181s 1s/step - loss: 2.0206 - accuracy: 0.2638 - rmse: 5.2531 - val_loss: 1.9718 - val_accuracy: 0.3084 - val_rmse: 5.2475\n",
      "Epoch 20/400\n",
      "167/167 [==============================] - 176s 1s/step - loss: 2.0091 - accuracy: 0.2713 - rmse: 5.2523 - val_loss: 1.9600 - val_accuracy: 0.3103 - val_rmse: 5.2475\n",
      "Epoch 21/400\n",
      "167/167 [==============================] - 177s 1s/step - loss: 1.9972 - accuracy: 0.2767 - rmse: 5.2528 - val_loss: 1.9486 - val_accuracy: 0.3187 - val_rmse: 5.2475\n",
      "Epoch 22/400\n",
      "167/167 [==============================] - 172s 1s/step - loss: 1.9915 - accuracy: 0.2800 - rmse: 5.2532 - val_loss: 1.9393 - val_accuracy: 0.3194 - val_rmse: 5.2476\n",
      "Epoch 23/400\n",
      "167/167 [==============================] - 172s 1s/step - loss: 1.9748 - accuracy: 0.2874 - rmse: 5.2529 - val_loss: 1.9266 - val_accuracy: 0.3257 - val_rmse: 5.2476\n",
      "Epoch 24/400\n",
      "167/167 [==============================] - 175s 1s/step - loss: 1.9692 - accuracy: 0.2901 - rmse: 5.2533 - val_loss: 1.9164 - val_accuracy: 0.3293 - val_rmse: 5.2476\n",
      "Epoch 25/400\n",
      "167/167 [==============================] - 175s 1s/step - loss: 1.9593 - accuracy: 0.2955 - rmse: 5.2522 - val_loss: 1.9065 - val_accuracy: 0.3358 - val_rmse: 5.2476\n",
      "Epoch 26/400\n",
      "167/167 [==============================] - 176s 1s/step - loss: 1.9488 - accuracy: 0.3017 - rmse: 5.2528 - val_loss: 1.8967 - val_accuracy: 0.3407 - val_rmse: 5.2476\n",
      "Epoch 27/400\n",
      "167/167 [==============================] - 179s 1s/step - loss: 1.9392 - accuracy: 0.3069 - rmse: 5.2534 - val_loss: 1.8879 - val_accuracy: 0.3453 - val_rmse: 5.2476\n",
      "Epoch 28/400\n",
      "167/167 [==============================] - 166s 992ms/step - loss: 1.9276 - accuracy: 0.3133 - rmse: 5.2537 - val_loss: 1.8752 - val_accuracy: 0.3477 - val_rmse: 5.2477\n",
      "Epoch 29/400\n",
      "167/167 [==============================] - 178s 1s/step - loss: 1.9221 - accuracy: 0.3112 - rmse: 5.2537 - val_loss: 1.8646 - val_accuracy: 0.3569 - val_rmse: 5.2477\n",
      "Epoch 30/400\n",
      "167/167 [==============================] - 180s 1s/step - loss: 1.9113 - accuracy: 0.3190 - rmse: 5.2525 - val_loss: 1.8565 - val_accuracy: 0.3569 - val_rmse: 5.2477\n",
      "Epoch 31/400\n",
      "167/167 [==============================] - 175s 1s/step - loss: 1.8968 - accuracy: 0.3229 - rmse: 5.2529 - val_loss: 1.8412 - val_accuracy: 0.3606 - val_rmse: 5.2478\n",
      "Epoch 32/400\n",
      "167/167 [==============================] - 173s 1s/step - loss: 1.8892 - accuracy: 0.3295 - rmse: 5.2537 - val_loss: 1.8310 - val_accuracy: 0.3679 - val_rmse: 5.2478\n",
      "Epoch 33/400\n",
      "167/167 [==============================] - 169s 1s/step - loss: 1.8789 - accuracy: 0.3334 - rmse: 5.2532 - val_loss: 1.8257 - val_accuracy: 0.3643 - val_rmse: 5.2478\n",
      "Epoch 34/400\n",
      "167/167 [==============================] - 188s 1s/step - loss: 1.8721 - accuracy: 0.3334 - rmse: 5.2533 - val_loss: 1.8133 - val_accuracy: 0.3772 - val_rmse: 5.2478\n",
      "Epoch 35/400\n",
      "167/167 [==============================] - 177s 1s/step - loss: 1.8557 - accuracy: 0.3428 - rmse: 5.2530 - val_loss: 1.7993 - val_accuracy: 0.3784 - val_rmse: 5.2479\n",
      "Epoch 36/400\n",
      "167/167 [==============================] - 165s 989ms/step - loss: 1.8492 - accuracy: 0.3435 - rmse: 5.2537 - val_loss: 1.7892 - val_accuracy: 0.3841 - val_rmse: 5.2479\n",
      "Epoch 37/400\n",
      "167/167 [==============================] - 169s 1s/step - loss: 1.8444 - accuracy: 0.3459 - rmse: 5.2542 - val_loss: 1.7817 - val_accuracy: 0.3828 - val_rmse: 5.2479\n",
      "Epoch 38/400\n",
      "167/167 [==============================] - 168s 1s/step - loss: 1.8349 - accuracy: 0.3494 - rmse: 5.2534 - val_loss: 1.7725 - val_accuracy: 0.3887 - val_rmse: 5.2479\n",
      "Epoch 39/400\n",
      "167/167 [==============================] - 177s 1s/step - loss: 1.8238 - accuracy: 0.3575 - rmse: 5.2532 - val_loss: 1.7606 - val_accuracy: 0.3925 - val_rmse: 5.2480\n",
      "Epoch 40/400\n",
      "167/167 [==============================] - 166s 995ms/step - loss: 1.8152 - accuracy: 0.3565 - rmse: 5.2531 - val_loss: 1.7498 - val_accuracy: 0.3952 - val_rmse: 5.2480\n",
      "Epoch 41/400\n",
      "167/167 [==============================] - 169s 1s/step - loss: 1.8057 - accuracy: 0.3628 - rmse: 5.2535 - val_loss: 1.7405 - val_accuracy: 0.3971 - val_rmse: 5.2480\n",
      "Epoch 42/400\n",
      "167/167 [==============================] - 175s 1s/step - loss: 1.8015 - accuracy: 0.3644 - rmse: 5.2532 - val_loss: 1.7375 - val_accuracy: 0.4014 - val_rmse: 5.2480\n",
      "Epoch 43/400\n",
      "167/167 [==============================] - 174s 1s/step - loss: 1.7904 - accuracy: 0.3679 - rmse: 5.2540 - val_loss: 1.7230 - val_accuracy: 0.4034 - val_rmse: 5.2481\n",
      "Epoch 44/400\n",
      "167/167 [==============================] - 167s 999ms/step - loss: 1.7832 - accuracy: 0.3723 - rmse: 5.2543 - val_loss: 1.7117 - val_accuracy: 0.4064 - val_rmse: 5.2481\n",
      "Epoch 45/400\n",
      "167/167 [==============================] - 173s 1s/step - loss: 1.7768 - accuracy: 0.3722 - rmse: 5.2540 - val_loss: 1.7091 - val_accuracy: 0.4086 - val_rmse: 5.2481\n",
      "Epoch 46/400\n",
      "167/167 [==============================] - 170s 1s/step - loss: 1.7680 - accuracy: 0.3748 - rmse: 5.2543 - val_loss: 1.6966 - val_accuracy: 0.4148 - val_rmse: 5.2481\n",
      "Epoch 47/400\n",
      "167/167 [==============================] - 164s 981ms/step - loss: 1.7610 - accuracy: 0.3783 - rmse: 5.2539 - val_loss: 1.6888 - val_accuracy: 0.4158 - val_rmse: 5.2481\n",
      "Epoch 48/400\n",
      "167/167 [==============================] - 176s 1s/step - loss: 1.7527 - accuracy: 0.3813 - rmse: 5.2534 - val_loss: 1.6818 - val_accuracy: 0.4147 - val_rmse: 5.2482\n",
      "Epoch 49/400\n",
      "167/167 [==============================] - 164s 981ms/step - loss: 1.7448 - accuracy: 0.3846 - rmse: 5.2535 - val_loss: 1.6740 - val_accuracy: 0.4238 - val_rmse: 5.2482\n",
      "Epoch 50/400\n",
      "167/167 [==============================] - 166s 993ms/step - loss: 1.7362 - accuracy: 0.3875 - rmse: 5.2541 - val_loss: 1.6677 - val_accuracy: 0.4230 - val_rmse: 5.2482\n",
      "Epoch 51/400\n",
      "167/167 [==============================] - 179s 1s/step - loss: 1.7292 - accuracy: 0.3921 - rmse: 5.2535 - val_loss: 1.6587 - val_accuracy: 0.4275 - val_rmse: 5.2482\n",
      "Epoch 52/400\n",
      "167/167 [==============================] - 171s 1s/step - loss: 1.7255 - accuracy: 0.3937 - rmse: 5.2539 - val_loss: 1.6546 - val_accuracy: 0.4280 - val_rmse: 5.2482\n",
      "Epoch 53/400\n",
      "167/167 [==============================] - 165s 987ms/step - loss: 1.7163 - accuracy: 0.3923 - rmse: 5.2541 - val_loss: 1.6423 - val_accuracy: 0.4309 - val_rmse: 5.2483\n",
      "Epoch 54/400\n",
      "167/167 [==============================] - 166s 993ms/step - loss: 1.7122 - accuracy: 0.3966 - rmse: 5.2539 - val_loss: 1.6403 - val_accuracy: 0.4346 - val_rmse: 5.2483\n",
      "Epoch 55/400\n",
      "167/167 [==============================] - 171s 1s/step - loss: 1.7002 - accuracy: 0.4002 - rmse: 5.2532 - val_loss: 1.6273 - val_accuracy: 0.4387 - val_rmse: 5.2483\n",
      "Epoch 56/400\n",
      "167/167 [==============================] - 173s 1s/step - loss: 1.6947 - accuracy: 0.4024 - rmse: 5.2537 - val_loss: 1.6181 - val_accuracy: 0.4393 - val_rmse: 5.2483\n",
      "Epoch 57/400\n",
      "167/167 [==============================] - 168s 1s/step - loss: 1.6896 - accuracy: 0.4037 - rmse: 5.2533 - val_loss: 1.6101 - val_accuracy: 0.4455 - val_rmse: 5.2483\n",
      "Epoch 58/400\n",
      "167/167 [==============================] - 180s 1s/step - loss: 1.6790 - accuracy: 0.4104 - rmse: 5.2546 - val_loss: 1.6041 - val_accuracy: 0.4429 - val_rmse: 5.2484\n",
      "Epoch 59/400\n",
      "167/167 [==============================] - 171s 1s/step - loss: 1.6713 - accuracy: 0.4099 - rmse: 5.2537 - val_loss: 1.5942 - val_accuracy: 0.4483 - val_rmse: 5.2484\n",
      "Epoch 60/400\n",
      "167/167 [==============================] - 172s 1s/step - loss: 1.6677 - accuracy: 0.4103 - rmse: 5.2539 - val_loss: 1.5880 - val_accuracy: 0.4497 - val_rmse: 5.2484\n",
      "Epoch 61/400\n",
      "167/167 [==============================] - 174s 1s/step - loss: 1.6614 - accuracy: 0.4129 - rmse: 5.2545 - val_loss: 1.5824 - val_accuracy: 0.4506 - val_rmse: 5.2484\n",
      "Epoch 62/400\n",
      "167/167 [==============================] - 165s 988ms/step - loss: 1.6526 - accuracy: 0.4151 - rmse: 5.2545 - val_loss: 1.5721 - val_accuracy: 0.4523 - val_rmse: 5.2485\n",
      "Epoch 63/400\n",
      "167/167 [==============================] - 165s 985ms/step - loss: 1.6458 - accuracy: 0.4185 - rmse: 5.2546 - val_loss: 1.5670 - val_accuracy: 0.4549 - val_rmse: 5.2485\n",
      "Epoch 64/400\n",
      "167/167 [==============================] - 163s 977ms/step - loss: 1.6401 - accuracy: 0.4192 - rmse: 5.2550 - val_loss: 1.5566 - val_accuracy: 0.4552 - val_rmse: 5.2485\n",
      "Epoch 65/400\n",
      "167/167 [==============================] - 163s 979ms/step - loss: 1.6322 - accuracy: 0.4222 - rmse: 5.2534 - val_loss: 1.5636 - val_accuracy: 0.4545 - val_rmse: 5.2486\n",
      "Epoch 66/400\n",
      "167/167 [==============================] - 166s 994ms/step - loss: 1.6274 - accuracy: 0.4243 - rmse: 5.2548 - val_loss: 1.5417 - val_accuracy: 0.4636 - val_rmse: 5.2486\n",
      "Epoch 67/400\n",
      "167/167 [==============================] - 162s 971ms/step - loss: 1.6223 - accuracy: 0.4254 - rmse: 5.2541 - val_loss: 1.5400 - val_accuracy: 0.4649 - val_rmse: 5.2486\n",
      "Epoch 68/400\n",
      "167/167 [==============================] - 154s 921ms/step - loss: 1.6195 - accuracy: 0.4265 - rmse: 5.2541 - val_loss: 1.5491 - val_accuracy: 0.4608 - val_rmse: 5.2486\n",
      "Epoch 69/400\n",
      "167/167 [==============================] - 152s 910ms/step - loss: 1.6090 - accuracy: 0.4292 - rmse: 5.2545 - val_loss: 1.5254 - val_accuracy: 0.4707 - val_rmse: 5.2486\n",
      "Epoch 70/400\n",
      "167/167 [==============================] - 152s 911ms/step - loss: 1.6039 - accuracy: 0.4318 - rmse: 5.2547 - val_loss: 1.5272 - val_accuracy: 0.4641 - val_rmse: 5.2487\n",
      "Epoch 71/400\n",
      "167/167 [==============================] - 151s 904ms/step - loss: 1.5979 - accuracy: 0.4339 - rmse: 5.2535 - val_loss: 1.5138 - val_accuracy: 0.4691 - val_rmse: 5.2487\n",
      "Epoch 72/400\n",
      "167/167 [==============================] - 151s 907ms/step - loss: 1.5903 - accuracy: 0.4363 - rmse: 5.2544 - val_loss: 1.5094 - val_accuracy: 0.4707 - val_rmse: 5.2487\n",
      "Epoch 73/400\n",
      "167/167 [==============================] - 152s 909ms/step - loss: 1.5812 - accuracy: 0.4388 - rmse: 5.2543 - val_loss: 1.5028 - val_accuracy: 0.4708 - val_rmse: 5.2486\n",
      "Epoch 74/400\n",
      "167/167 [==============================] - 152s 910ms/step - loss: 1.5819 - accuracy: 0.4399 - rmse: 5.2541 - val_loss: 1.4970 - val_accuracy: 0.4739 - val_rmse: 5.2487\n",
      "Epoch 75/400\n",
      "167/167 [==============================] - 151s 904ms/step - loss: 1.5709 - accuracy: 0.4440 - rmse: 5.2538 - val_loss: 1.4872 - val_accuracy: 0.4780 - val_rmse: 5.2487\n",
      "Epoch 76/400\n",
      "167/167 [==============================] - 152s 909ms/step - loss: 1.5710 - accuracy: 0.4442 - rmse: 5.2545 - val_loss: 1.4813 - val_accuracy: 0.4809 - val_rmse: 5.2488\n",
      "Epoch 77/400\n",
      "167/167 [==============================] - 151s 903ms/step - loss: 1.5647 - accuracy: 0.4417 - rmse: 5.2547 - val_loss: 1.4864 - val_accuracy: 0.4770 - val_rmse: 5.2487\n",
      "Epoch 78/400\n",
      "167/167 [==============================] - 152s 909ms/step - loss: 1.5560 - accuracy: 0.4490 - rmse: 5.2545 - val_loss: 1.4710 - val_accuracy: 0.4806 - val_rmse: 5.2488\n",
      "Epoch 79/400\n",
      "167/167 [==============================] - 150s 901ms/step - loss: 1.5526 - accuracy: 0.4509 - rmse: 5.2540 - val_loss: 1.4689 - val_accuracy: 0.4829 - val_rmse: 5.2488\n",
      "Epoch 80/400\n",
      "167/167 [==============================] - 152s 908ms/step - loss: 1.5471 - accuracy: 0.4492 - rmse: 5.2543 - val_loss: 1.4632 - val_accuracy: 0.4849 - val_rmse: 5.2488\n",
      "Epoch 81/400\n",
      "167/167 [==============================] - 151s 907ms/step - loss: 1.5439 - accuracy: 0.4515 - rmse: 5.2541 - val_loss: 1.4616 - val_accuracy: 0.4870 - val_rmse: 5.2488\n",
      "Epoch 82/400\n",
      "167/167 [==============================] - 151s 903ms/step - loss: 1.5395 - accuracy: 0.4558 - rmse: 5.2540 - val_loss: 1.4666 - val_accuracy: 0.4844 - val_rmse: 5.2488\n",
      "Epoch 83/400\n",
      "167/167 [==============================] - 150s 901ms/step - loss: 1.5324 - accuracy: 0.4567 - rmse: 5.2553 - val_loss: 1.5495 - val_accuracy: 0.4500 - val_rmse: 5.2489\n",
      "Epoch 84/400\n",
      "167/167 [==============================] - 150s 901ms/step - loss: 1.5338 - accuracy: 0.4573 - rmse: 5.2549 - val_loss: 1.4637 - val_accuracy: 0.4809 - val_rmse: 5.2489\n",
      "Epoch 85/400\n",
      "167/167 [==============================] - 151s 905ms/step - loss: 1.5248 - accuracy: 0.4594 - rmse: 5.2546 - val_loss: 1.4489 - val_accuracy: 0.4909 - val_rmse: 5.2488\n",
      "Epoch 86/400\n",
      "167/167 [==============================] - 152s 913ms/step - loss: 1.5249 - accuracy: 0.4601 - rmse: 5.2543 - val_loss: 1.4352 - val_accuracy: 0.4962 - val_rmse: 5.2489\n",
      "Epoch 87/400\n",
      "167/167 [==============================] - 151s 905ms/step - loss: 1.5159 - accuracy: 0.4636 - rmse: 5.2547 - val_loss: 1.4577 - val_accuracy: 0.4833 - val_rmse: 5.2490\n",
      "Epoch 88/400\n",
      "167/167 [==============================] - 151s 903ms/step - loss: 1.5128 - accuracy: 0.4634 - rmse: 5.2536 - val_loss: 1.4382 - val_accuracy: 0.4925 - val_rmse: 5.2489\n",
      "Epoch 89/400\n",
      "167/167 [==============================] - 151s 903ms/step - loss: 1.5090 - accuracy: 0.4627 - rmse: 5.2548 - val_loss: 1.4248 - val_accuracy: 0.4984 - val_rmse: 5.2489\n",
      "Epoch 90/400\n",
      "167/167 [==============================] - 152s 912ms/step - loss: 1.5048 - accuracy: 0.4659 - rmse: 5.2545 - val_loss: 1.4337 - val_accuracy: 0.4930 - val_rmse: 5.2489\n",
      "Epoch 91/400\n",
      "167/167 [==============================] - 152s 910ms/step - loss: 1.4994 - accuracy: 0.4668 - rmse: 5.2550 - val_loss: 1.4148 - val_accuracy: 0.5035 - val_rmse: 5.2490\n",
      "Epoch 92/400\n",
      "167/167 [==============================] - 152s 909ms/step - loss: 1.4963 - accuracy: 0.4691 - rmse: 5.2547 - val_loss: 1.4374 - val_accuracy: 0.4933 - val_rmse: 5.2490\n",
      "Epoch 93/400\n",
      "167/167 [==============================] - 153s 914ms/step - loss: 1.4906 - accuracy: 0.4699 - rmse: 5.2546 - val_loss: 1.4203 - val_accuracy: 0.4972 - val_rmse: 5.2490\n",
      "Epoch 94/400\n",
      "167/167 [==============================] - 151s 906ms/step - loss: 1.4862 - accuracy: 0.4732 - rmse: 5.2547 - val_loss: 1.4074 - val_accuracy: 0.5049 - val_rmse: 5.2490\n",
      "Epoch 95/400\n",
      "167/167 [==============================] - 151s 904ms/step - loss: 1.4864 - accuracy: 0.4726 - rmse: 5.2541 - val_loss: 1.4244 - val_accuracy: 0.5002 - val_rmse: 5.2490\n",
      "Epoch 96/400\n",
      "167/167 [==============================] - 152s 913ms/step - loss: 1.4807 - accuracy: 0.4740 - rmse: 5.2540 - val_loss: 1.4552 - val_accuracy: 0.4856 - val_rmse: 5.2490\n",
      "Epoch 97/400\n",
      "167/167 [==============================] - 151s 906ms/step - loss: 1.4796 - accuracy: 0.4772 - rmse: 5.2547 - val_loss: 1.3914 - val_accuracy: 0.5093 - val_rmse: 5.2491\n",
      "Epoch 98/400\n",
      "167/167 [==============================] - 151s 907ms/step - loss: 1.4706 - accuracy: 0.4767 - rmse: 5.2544 - val_loss: 1.4055 - val_accuracy: 0.5044 - val_rmse: 5.2491\n",
      "Epoch 99/400\n",
      "167/167 [==============================] - 151s 905ms/step - loss: 1.4705 - accuracy: 0.4792 - rmse: 5.2548 - val_loss: 1.3918 - val_accuracy: 0.5102 - val_rmse: 5.2490\n",
      "Epoch 100/400\n",
      "167/167 [==============================] - 151s 905ms/step - loss: 1.4640 - accuracy: 0.4838 - rmse: 5.2541 - val_loss: 1.3906 - val_accuracy: 0.5112 - val_rmse: 5.2491\n",
      "Epoch 101/400\n",
      "167/167 [==============================] - 152s 909ms/step - loss: 1.4623 - accuracy: 0.4820 - rmse: 5.2547 - val_loss: 1.3840 - val_accuracy: 0.5111 - val_rmse: 5.2491\n",
      "Epoch 102/400\n",
      "167/167 [==============================] - 151s 903ms/step - loss: 1.4589 - accuracy: 0.4838 - rmse: 5.2552 - val_loss: 1.3891 - val_accuracy: 0.5103 - val_rmse: 5.2491\n",
      "Epoch 103/400\n",
      "167/167 [==============================] - 150s 900ms/step - loss: 1.4549 - accuracy: 0.4834 - rmse: 5.2549 - val_loss: 1.3860 - val_accuracy: 0.5135 - val_rmse: 5.2491\n",
      "Epoch 104/400\n",
      "167/167 [==============================] - 151s 902ms/step - loss: 1.4522 - accuracy: 0.4851 - rmse: 5.2544 - val_loss: 1.3947 - val_accuracy: 0.5032 - val_rmse: 5.2491\n",
      "Epoch 105/400\n",
      "167/167 [==============================] - 151s 905ms/step - loss: 1.4480 - accuracy: 0.4884 - rmse: 5.2549 - val_loss: 1.3873 - val_accuracy: 0.5039 - val_rmse: 5.2493\n",
      "Epoch 106/400\n",
      "167/167 [==============================] - 150s 899ms/step - loss: 1.4467 - accuracy: 0.4874 - rmse: 5.2550 - val_loss: 1.3690 - val_accuracy: 0.5158 - val_rmse: 5.2492\n",
      "Epoch 107/400\n",
      "167/167 [==============================] - 150s 900ms/step - loss: 1.4389 - accuracy: 0.4913 - rmse: 5.2538 - val_loss: 1.3571 - val_accuracy: 0.5194 - val_rmse: 5.2492\n",
      "Epoch 108/400\n",
      "167/167 [==============================] - 150s 898ms/step - loss: 1.4371 - accuracy: 0.4919 - rmse: 5.2551 - val_loss: 1.3580 - val_accuracy: 0.5210 - val_rmse: 5.2492\n",
      "Epoch 109/400\n",
      "167/167 [==============================] - 152s 912ms/step - loss: 1.4355 - accuracy: 0.4906 - rmse: 5.2540 - val_loss: 1.3588 - val_accuracy: 0.5206 - val_rmse: 5.2492\n",
      "Epoch 110/400\n",
      "167/167 [==============================] - 151s 905ms/step - loss: 1.4310 - accuracy: 0.4901 - rmse: 5.2549 - val_loss: 1.3541 - val_accuracy: 0.5233 - val_rmse: 5.2493\n",
      "Epoch 111/400\n",
      "167/167 [==============================] - 151s 906ms/step - loss: 1.4290 - accuracy: 0.4947 - rmse: 5.2551 - val_loss: 1.3549 - val_accuracy: 0.5217 - val_rmse: 5.2492\n",
      "Epoch 112/400\n",
      "167/167 [==============================] - 150s 900ms/step - loss: 1.4264 - accuracy: 0.4936 - rmse: 5.2547 - val_loss: 1.3516 - val_accuracy: 0.5202 - val_rmse: 5.2493\n",
      "Epoch 113/400\n",
      "167/167 [==============================] - 151s 907ms/step - loss: 1.4232 - accuracy: 0.4960 - rmse: 5.2547 - val_loss: 1.3481 - val_accuracy: 0.5248 - val_rmse: 5.2492\n",
      "Epoch 114/400\n",
      "167/167 [==============================] - 155s 929ms/step - loss: 1.4184 - accuracy: 0.4960 - rmse: 5.2549 - val_loss: 1.3599 - val_accuracy: 0.5162 - val_rmse: 5.2493\n",
      "Epoch 115/400\n",
      "167/167 [==============================] - 158s 948ms/step - loss: 1.4146 - accuracy: 0.4981 - rmse: 5.2547 - val_loss: 1.3406 - val_accuracy: 0.5248 - val_rmse: 5.2493\n",
      "Epoch 116/400\n",
      "167/167 [==============================] - 151s 903ms/step - loss: 1.4149 - accuracy: 0.4996 - rmse: 5.2550 - val_loss: 1.3357 - val_accuracy: 0.5236 - val_rmse: 5.2494\n",
      "Epoch 117/400\n",
      "167/167 [==============================] - 152s 911ms/step - loss: 1.4131 - accuracy: 0.4988 - rmse: 5.2548 - val_loss: 1.3341 - val_accuracy: 0.5299 - val_rmse: 5.2493\n",
      "Epoch 118/400\n",
      "167/167 [==============================] - 151s 906ms/step - loss: 1.4047 - accuracy: 0.5004 - rmse: 5.2554 - val_loss: 1.3414 - val_accuracy: 0.5223 - val_rmse: 5.2494\n",
      "Epoch 119/400\n",
      "167/167 [==============================] - 151s 904ms/step - loss: 1.4041 - accuracy: 0.5035 - rmse: 5.2544 - val_loss: 1.3308 - val_accuracy: 0.5311 - val_rmse: 5.2493\n",
      "Epoch 120/400\n",
      "167/167 [==============================] - 151s 904ms/step - loss: 1.4020 - accuracy: 0.5043 - rmse: 5.2545 - val_loss: 1.3273 - val_accuracy: 0.5311 - val_rmse: 5.2494\n",
      "Epoch 121/400\n",
      "167/167 [==============================] - 152s 908ms/step - loss: 1.4014 - accuracy: 0.5049 - rmse: 5.2551 - val_loss: 1.3343 - val_accuracy: 0.5263 - val_rmse: 5.2494\n",
      "Epoch 122/400\n",
      "167/167 [==============================] - 151s 902ms/step - loss: 1.3982 - accuracy: 0.5047 - rmse: 5.2550 - val_loss: 1.3675 - val_accuracy: 0.5179 - val_rmse: 5.2494\n",
      "Epoch 123/400\n",
      "167/167 [==============================] - 152s 909ms/step - loss: 1.3937 - accuracy: 0.5069 - rmse: 5.2542 - val_loss: 1.3213 - val_accuracy: 0.5344 - val_rmse: 5.2494\n",
      "Epoch 124/400\n",
      "167/167 [==============================] - 151s 905ms/step - loss: 1.3921 - accuracy: 0.5083 - rmse: 5.2545 - val_loss: 1.3156 - val_accuracy: 0.5370 - val_rmse: 5.2494\n",
      "Epoch 125/400\n",
      "167/167 [==============================] - 151s 904ms/step - loss: 1.3881 - accuracy: 0.5078 - rmse: 5.2550 - val_loss: 1.3181 - val_accuracy: 0.5318 - val_rmse: 5.2494\n",
      "Epoch 126/400\n",
      "167/167 [==============================] - 152s 908ms/step - loss: 1.3885 - accuracy: 0.5061 - rmse: 5.2559 - val_loss: 1.3226 - val_accuracy: 0.5264 - val_rmse: 5.2495\n",
      "Epoch 127/400\n",
      "167/167 [==============================] - 150s 901ms/step - loss: 1.3849 - accuracy: 0.5119 - rmse: 5.2538 - val_loss: 1.3406 - val_accuracy: 0.5254 - val_rmse: 5.2493\n",
      "Epoch 128/400\n",
      "167/167 [==============================] - 152s 908ms/step - loss: 1.3802 - accuracy: 0.5129 - rmse: 5.2542 - val_loss: 1.3402 - val_accuracy: 0.5259 - val_rmse: 5.2494\n",
      "Epoch 129/400\n",
      "167/167 [==============================] - 152s 909ms/step - loss: 1.3761 - accuracy: 0.5136 - rmse: 5.2552 - val_loss: 1.3139 - val_accuracy: 0.5352 - val_rmse: 5.2494\n",
      "Epoch 130/400\n",
      "167/167 [==============================] - 151s 903ms/step - loss: 1.3786 - accuracy: 0.5110 - rmse: 5.2552 - val_loss: 1.3039 - val_accuracy: 0.5415 - val_rmse: 5.2494\n",
      "Epoch 131/400\n",
      "167/167 [==============================] - 149s 894ms/step - loss: 1.3761 - accuracy: 0.5134 - rmse: 5.2550 - val_loss: 1.3005 - val_accuracy: 0.5406 - val_rmse: 5.2495\n",
      "Epoch 132/400\n",
      "167/167 [==============================] - 151s 905ms/step - loss: 1.3696 - accuracy: 0.5157 - rmse: 5.2550 - val_loss: 1.3112 - val_accuracy: 0.5387 - val_rmse: 5.2494\n",
      "Epoch 133/400\n",
      "167/167 [==============================] - 151s 902ms/step - loss: 1.3663 - accuracy: 0.5196 - rmse: 5.2548 - val_loss: 1.3237 - val_accuracy: 0.5373 - val_rmse: 5.2494\n",
      "Epoch 134/400\n",
      "167/167 [==============================] - 151s 906ms/step - loss: 1.3648 - accuracy: 0.5165 - rmse: 5.2547 - val_loss: 1.3099 - val_accuracy: 0.5403 - val_rmse: 5.2494\n",
      "Epoch 135/400\n",
      "167/167 [==============================] - 151s 906ms/step - loss: 1.3581 - accuracy: 0.5198 - rmse: 5.2548 - val_loss: 1.2969 - val_accuracy: 0.5424 - val_rmse: 5.2495\n",
      "Epoch 136/400\n",
      "167/167 [==============================] - 151s 903ms/step - loss: 1.3585 - accuracy: 0.5194 - rmse: 5.2543 - val_loss: 1.3038 - val_accuracy: 0.5409 - val_rmse: 5.2494\n",
      "Epoch 137/400\n",
      "167/167 [==============================] - 151s 907ms/step - loss: 1.3590 - accuracy: 0.5182 - rmse: 5.2551 - val_loss: 1.2994 - val_accuracy: 0.5437 - val_rmse: 5.2494\n",
      "Epoch 138/400\n",
      "167/167 [==============================] - 154s 923ms/step - loss: 1.3564 - accuracy: 0.5211 - rmse: 5.2542 - val_loss: 1.2875 - val_accuracy: 0.5464 - val_rmse: 5.2495\n",
      "Epoch 139/400\n",
      "167/167 [==============================] - 153s 914ms/step - loss: 1.3573 - accuracy: 0.5209 - rmse: 5.2549 - val_loss: 1.2872 - val_accuracy: 0.5429 - val_rmse: 5.2496\n",
      "Epoch 140/400\n",
      "167/167 [==============================] - 150s 900ms/step - loss: 1.3492 - accuracy: 0.5218 - rmse: 5.2549 - val_loss: 1.2798 - val_accuracy: 0.5488 - val_rmse: 5.2495\n",
      "Epoch 141/400\n",
      "167/167 [==============================] - 151s 905ms/step - loss: 1.3468 - accuracy: 0.5250 - rmse: 5.2552 - val_loss: 1.2822 - val_accuracy: 0.5486 - val_rmse: 5.2496\n",
      "Epoch 142/400\n",
      "167/167 [==============================] - 151s 904ms/step - loss: 1.3443 - accuracy: 0.5257 - rmse: 5.2554 - val_loss: 1.2980 - val_accuracy: 0.5392 - val_rmse: 5.2497\n",
      "Epoch 143/400\n",
      "167/167 [==============================] - 151s 903ms/step - loss: 1.3444 - accuracy: 0.5265 - rmse: 5.2550 - val_loss: 1.2833 - val_accuracy: 0.5481 - val_rmse: 5.2495\n",
      "Epoch 144/400\n",
      "167/167 [==============================] - 158s 947ms/step - loss: 1.3430 - accuracy: 0.5260 - rmse: 5.2556 - val_loss: 1.2850 - val_accuracy: 0.5442 - val_rmse: 5.2496\n",
      "Epoch 145/400\n",
      "167/167 [==============================] - 151s 905ms/step - loss: 1.3395 - accuracy: 0.5282 - rmse: 5.2556 - val_loss: 1.2831 - val_accuracy: 0.5449 - val_rmse: 5.2495\n",
      "Epoch 146/400\n",
      "167/167 [==============================] - 150s 901ms/step - loss: 1.3382 - accuracy: 0.5287 - rmse: 5.2552 - val_loss: 1.2687 - val_accuracy: 0.5496 - val_rmse: 5.2497\n",
      "Epoch 147/400\n",
      "167/167 [==============================] - 151s 904ms/step - loss: 1.3352 - accuracy: 0.5274 - rmse: 5.2544 - val_loss: 1.2758 - val_accuracy: 0.5504 - val_rmse: 5.2495\n",
      "Epoch 148/400\n",
      "167/167 [==============================] - 156s 934ms/step - loss: 1.3331 - accuracy: 0.5285 - rmse: 5.2546 - val_loss: 1.2821 - val_accuracy: 0.5483 - val_rmse: 5.2496\n",
      "Epoch 149/400\n",
      "167/167 [==============================] - 169s 1s/step - loss: 1.3342 - accuracy: 0.5295 - rmse: 5.2539 - val_loss: 1.3043 - val_accuracy: 0.5377 - val_rmse: 5.2495\n",
      "Epoch 150/400\n",
      "167/167 [==============================] - 180s 1s/step - loss: 1.3313 - accuracy: 0.5292 - rmse: 5.2550 - val_loss: 1.2667 - val_accuracy: 0.5512 - val_rmse: 5.2496\n",
      "Epoch 151/400\n",
      "167/167 [==============================] - 173s 1s/step - loss: 1.3254 - accuracy: 0.5346 - rmse: 5.2560 - val_loss: 1.2631 - val_accuracy: 0.5542 - val_rmse: 5.2497\n",
      "Epoch 152/400\n",
      "167/167 [==============================] - 160s 956ms/step - loss: 1.3264 - accuracy: 0.5323 - rmse: 5.2554 - val_loss: 1.2629 - val_accuracy: 0.5534 - val_rmse: 5.2496\n",
      "Epoch 153/400\n",
      "167/167 [==============================] - 163s 979ms/step - loss: 1.3238 - accuracy: 0.5341 - rmse: 5.2555 - val_loss: 1.2646 - val_accuracy: 0.5528 - val_rmse: 5.2497\n",
      "Epoch 154/400\n",
      "167/167 [==============================] - 166s 992ms/step - loss: 1.3204 - accuracy: 0.5348 - rmse: 5.2549 - val_loss: 1.2568 - val_accuracy: 0.5569 - val_rmse: 5.2496\n",
      "Epoch 155/400\n",
      "167/167 [==============================] - 161s 964ms/step - loss: 1.3186 - accuracy: 0.5322 - rmse: 5.2555 - val_loss: 1.2588 - val_accuracy: 0.5566 - val_rmse: 5.2497\n",
      "Epoch 156/400\n",
      "167/167 [==============================] - 166s 994ms/step - loss: 1.3167 - accuracy: 0.5358 - rmse: 5.2552 - val_loss: 1.2551 - val_accuracy: 0.5554 - val_rmse: 5.2497\n",
      "Epoch 157/400\n",
      "167/167 [==============================] - 166s 993ms/step - loss: 1.3116 - accuracy: 0.5357 - rmse: 5.2554 - val_loss: 1.2580 - val_accuracy: 0.5599 - val_rmse: 5.2496\n",
      "Epoch 158/400\n",
      "167/167 [==============================] - 166s 996ms/step - loss: 1.3114 - accuracy: 0.5380 - rmse: 5.2557 - val_loss: 1.2512 - val_accuracy: 0.5621 - val_rmse: 5.2496\n",
      "Epoch 159/400\n",
      "167/167 [==============================] - 160s 955ms/step - loss: 1.3109 - accuracy: 0.5396 - rmse: 5.2544 - val_loss: 1.2494 - val_accuracy: 0.5581 - val_rmse: 5.2498\n",
      "Epoch 160/400\n",
      "167/167 [==============================] - 158s 949ms/step - loss: 1.3048 - accuracy: 0.5399 - rmse: 5.2554 - val_loss: 1.2567 - val_accuracy: 0.5508 - val_rmse: 5.2498\n",
      "Epoch 161/400\n",
      "167/167 [==============================] - 162s 972ms/step - loss: 1.3067 - accuracy: 0.5427 - rmse: 5.2557 - val_loss: 1.2485 - val_accuracy: 0.5620 - val_rmse: 5.2497\n",
      "Epoch 162/400\n",
      "167/167 [==============================] - 169s 1s/step - loss: 1.3046 - accuracy: 0.5404 - rmse: 5.2550 - val_loss: 1.2404 - val_accuracy: 0.5623 - val_rmse: 5.2497\n",
      "Epoch 163/400\n",
      "167/167 [==============================] - 162s 972ms/step - loss: 1.3024 - accuracy: 0.5405 - rmse: 5.2559 - val_loss: 1.2616 - val_accuracy: 0.5530 - val_rmse: 5.2498\n",
      "Epoch 164/400\n",
      "167/167 [==============================] - 152s 913ms/step - loss: 1.2972 - accuracy: 0.5451 - rmse: 5.2553 - val_loss: 1.2390 - val_accuracy: 0.5657 - val_rmse: 5.2497\n",
      "Epoch 165/400\n",
      "167/167 [==============================] - 152s 909ms/step - loss: 1.2970 - accuracy: 0.5417 - rmse: 5.2546 - val_loss: 1.2445 - val_accuracy: 0.5617 - val_rmse: 5.2498\n",
      "Epoch 166/400\n",
      "167/167 [==============================] - 151s 905ms/step - loss: 1.2945 - accuracy: 0.5444 - rmse: 5.2554 - val_loss: 1.2409 - val_accuracy: 0.5637 - val_rmse: 5.2497\n",
      "Epoch 167/400\n",
      "167/167 [==============================] - 150s 900ms/step - loss: 1.2946 - accuracy: 0.5435 - rmse: 5.2552 - val_loss: 1.2375 - val_accuracy: 0.5651 - val_rmse: 5.2497\n",
      "Epoch 168/400\n",
      "167/167 [==============================] - 150s 900ms/step - loss: 1.2907 - accuracy: 0.5462 - rmse: 5.2554 - val_loss: 1.2358 - val_accuracy: 0.5682 - val_rmse: 5.2497\n",
      "Epoch 169/400\n",
      "167/167 [==============================] - 151s 905ms/step - loss: 1.2874 - accuracy: 0.5471 - rmse: 5.2555 - val_loss: 1.2354 - val_accuracy: 0.5623 - val_rmse: 5.2498\n",
      "Epoch 170/400\n",
      "167/167 [==============================] - 151s 902ms/step - loss: 1.2861 - accuracy: 0.5463 - rmse: 5.2556 - val_loss: 1.2328 - val_accuracy: 0.5648 - val_rmse: 5.2498\n",
      "Epoch 171/400\n",
      "167/167 [==============================] - 151s 902ms/step - loss: 1.2850 - accuracy: 0.5483 - rmse: 5.2553 - val_loss: 1.2268 - val_accuracy: 0.5675 - val_rmse: 5.2498\n",
      "Epoch 172/400\n",
      "167/167 [==============================] - 151s 903ms/step - loss: 1.2816 - accuracy: 0.5479 - rmse: 5.2554 - val_loss: 1.2219 - val_accuracy: 0.5697 - val_rmse: 5.2498\n",
      "Epoch 173/400\n",
      "167/167 [==============================] - 150s 897ms/step - loss: 1.2787 - accuracy: 0.5477 - rmse: 5.2555 - val_loss: 1.2338 - val_accuracy: 0.5658 - val_rmse: 5.2498\n",
      "Epoch 174/400\n",
      "167/167 [==============================] - 150s 896ms/step - loss: 1.2728 - accuracy: 0.5537 - rmse: 5.2548 - val_loss: 1.2260 - val_accuracy: 0.5694 - val_rmse: 5.2498\n",
      "Epoch 175/400\n",
      "167/167 [==============================] - 150s 898ms/step - loss: 1.2766 - accuracy: 0.5500 - rmse: 5.2556 - val_loss: 1.2308 - val_accuracy: 0.5700 - val_rmse: 5.2498\n",
      "Epoch 176/400\n",
      "167/167 [==============================] - 151s 905ms/step - loss: 1.2760 - accuracy: 0.5502 - rmse: 5.2556 - val_loss: 1.2207 - val_accuracy: 0.5682 - val_rmse: 5.2498\n",
      "Epoch 177/400\n",
      "167/167 [==============================] - 150s 900ms/step - loss: 1.2713 - accuracy: 0.5545 - rmse: 5.2549 - val_loss: 1.2425 - val_accuracy: 0.5672 - val_rmse: 5.2498\n",
      "Epoch 178/400\n",
      "167/167 [==============================] - 150s 900ms/step - loss: 1.2684 - accuracy: 0.5506 - rmse: 5.2552 - val_loss: 1.2277 - val_accuracy: 0.5663 - val_rmse: 5.2497\n",
      "Epoch 179/400\n",
      "167/167 [==============================] - 150s 896ms/step - loss: 1.2673 - accuracy: 0.5553 - rmse: 5.2554 - val_loss: 1.2134 - val_accuracy: 0.5700 - val_rmse: 5.2498\n",
      "Epoch 180/400\n",
      "167/167 [==============================] - 150s 896ms/step - loss: 1.2625 - accuracy: 0.5582 - rmse: 5.2555 - val_loss: 1.2465 - val_accuracy: 0.5644 - val_rmse: 5.2498\n",
      "Epoch 181/400\n",
      "167/167 [==============================] - 151s 906ms/step - loss: 1.2673 - accuracy: 0.5550 - rmse: 5.2552 - val_loss: 1.2144 - val_accuracy: 0.5776 - val_rmse: 5.2498\n",
      "Epoch 182/400\n",
      "167/167 [==============================] - 151s 902ms/step - loss: 1.2621 - accuracy: 0.5566 - rmse: 5.2551 - val_loss: 1.2085 - val_accuracy: 0.5744 - val_rmse: 5.2499\n",
      "Epoch 183/400\n",
      "167/167 [==============================] - 151s 903ms/step - loss: 1.2590 - accuracy: 0.5582 - rmse: 5.2554 - val_loss: 1.2041 - val_accuracy: 0.5795 - val_rmse: 5.2499\n",
      "Epoch 184/400\n",
      "167/167 [==============================] - 152s 908ms/step - loss: 1.2571 - accuracy: 0.5577 - rmse: 5.2555 - val_loss: 1.2029 - val_accuracy: 0.5780 - val_rmse: 5.2499\n",
      "Epoch 185/400\n",
      "167/167 [==============================] - 153s 916ms/step - loss: 1.2563 - accuracy: 0.5611 - rmse: 5.2555 - val_loss: 1.2014 - val_accuracy: 0.5785 - val_rmse: 5.2499\n",
      "Epoch 186/400\n",
      "167/167 [==============================] - 151s 905ms/step - loss: 1.2518 - accuracy: 0.5591 - rmse: 5.2553 - val_loss: 1.2389 - val_accuracy: 0.5665 - val_rmse: 5.2498\n",
      "Epoch 187/400\n",
      "167/167 [==============================] - 153s 919ms/step - loss: 1.2520 - accuracy: 0.5580 - rmse: 5.2554 - val_loss: 1.2116 - val_accuracy: 0.5721 - val_rmse: 5.2500\n",
      "Epoch 188/400\n",
      "167/167 [==============================] - 151s 901ms/step - loss: 1.2477 - accuracy: 0.5590 - rmse: 5.2561 - val_loss: 1.2119 - val_accuracy: 0.5698 - val_rmse: 5.2499\n",
      "Epoch 189/400\n",
      "167/167 [==============================] - 152s 908ms/step - loss: 1.2510 - accuracy: 0.5601 - rmse: 5.2559 - val_loss: 1.2011 - val_accuracy: 0.5798 - val_rmse: 5.2499\n",
      "Epoch 190/400\n",
      "167/167 [==============================] - 150s 898ms/step - loss: 1.2408 - accuracy: 0.5619 - rmse: 5.2554 - val_loss: 1.2022 - val_accuracy: 0.5743 - val_rmse: 5.2500\n",
      "Epoch 191/400\n",
      "167/167 [==============================] - 151s 906ms/step - loss: 1.2464 - accuracy: 0.5609 - rmse: 5.2554 - val_loss: 1.1966 - val_accuracy: 0.5815 - val_rmse: 5.2499\n",
      "Epoch 192/400\n",
      "167/167 [==============================] - 150s 895ms/step - loss: 1.2420 - accuracy: 0.5632 - rmse: 5.2548 - val_loss: 1.1973 - val_accuracy: 0.5821 - val_rmse: 5.2499\n",
      "Epoch 193/400\n",
      "167/167 [==============================] - 150s 901ms/step - loss: 1.2411 - accuracy: 0.5628 - rmse: 5.2550 - val_loss: 1.2168 - val_accuracy: 0.5769 - val_rmse: 5.2499\n",
      "Epoch 194/400\n",
      "167/167 [==============================] - 152s 910ms/step - loss: 1.2395 - accuracy: 0.5647 - rmse: 5.2557 - val_loss: 1.1904 - val_accuracy: 0.5826 - val_rmse: 5.2499\n",
      "Epoch 195/400\n",
      "167/167 [==============================] - 150s 900ms/step - loss: 1.2366 - accuracy: 0.5661 - rmse: 5.2547 - val_loss: 1.1927 - val_accuracy: 0.5835 - val_rmse: 5.2499\n",
      "Epoch 196/400\n",
      "167/167 [==============================] - 150s 896ms/step - loss: 1.2327 - accuracy: 0.5675 - rmse: 5.2554 - val_loss: 1.1827 - val_accuracy: 0.5871 - val_rmse: 5.2500\n",
      "Epoch 197/400\n",
      "167/167 [==============================] - 150s 900ms/step - loss: 1.2300 - accuracy: 0.5659 - rmse: 5.2553 - val_loss: 1.1978 - val_accuracy: 0.5811 - val_rmse: 5.2500\n",
      "Epoch 198/400\n",
      "167/167 [==============================] - 151s 904ms/step - loss: 1.2321 - accuracy: 0.5676 - rmse: 5.2559 - val_loss: 1.1810 - val_accuracy: 0.5846 - val_rmse: 5.2500\n",
      "Epoch 199/400\n",
      "167/167 [==============================] - 150s 899ms/step - loss: 1.2263 - accuracy: 0.5690 - rmse: 5.2559 - val_loss: 1.1953 - val_accuracy: 0.5793 - val_rmse: 5.2500\n",
      "Epoch 200/400\n",
      "167/167 [==============================] - 151s 905ms/step - loss: 1.2263 - accuracy: 0.5686 - rmse: 5.2559 - val_loss: 1.1838 - val_accuracy: 0.5889 - val_rmse: 5.2499\n",
      "Epoch 201/400\n",
      "167/167 [==============================] - 151s 902ms/step - loss: 1.2305 - accuracy: 0.5673 - rmse: 5.2565 - val_loss: 1.1732 - val_accuracy: 0.5888 - val_rmse: 5.2500\n",
      "Epoch 202/400\n",
      "167/167 [==============================] - 151s 904ms/step - loss: 1.2192 - accuracy: 0.5719 - rmse: 5.2552 - val_loss: 1.1883 - val_accuracy: 0.5850 - val_rmse: 5.2500\n",
      "Epoch 203/400\n",
      "167/167 [==============================] - 150s 901ms/step - loss: 1.2201 - accuracy: 0.5710 - rmse: 5.2552 - val_loss: 1.1893 - val_accuracy: 0.5868 - val_rmse: 5.2500\n",
      "Epoch 204/400\n",
      "167/167 [==============================] - 151s 904ms/step - loss: 1.2177 - accuracy: 0.5715 - rmse: 5.2555 - val_loss: 1.1759 - val_accuracy: 0.5870 - val_rmse: 5.2501\n",
      "Epoch 205/400\n",
      "167/167 [==============================] - 150s 901ms/step - loss: 1.2182 - accuracy: 0.5726 - rmse: 5.2553 - val_loss: 1.2321 - val_accuracy: 0.5585 - val_rmse: 5.2502\n",
      "Epoch 206/400\n",
      "167/167 [==============================] - 151s 906ms/step - loss: 1.2185 - accuracy: 0.5704 - rmse: 5.2544 - val_loss: 1.1681 - val_accuracy: 0.5929 - val_rmse: 5.2500\n",
      "Epoch 207/400\n",
      "167/167 [==============================] - 151s 906ms/step - loss: 1.2145 - accuracy: 0.5753 - rmse: 5.2551 - val_loss: 1.1708 - val_accuracy: 0.5874 - val_rmse: 5.2500\n",
      "Epoch 208/400\n",
      "167/167 [==============================] - 151s 906ms/step - loss: 1.2139 - accuracy: 0.5748 - rmse: 5.2545 - val_loss: 1.1976 - val_accuracy: 0.5811 - val_rmse: 5.2500\n",
      "Epoch 209/400\n",
      "167/167 [==============================] - 154s 925ms/step - loss: 1.2092 - accuracy: 0.5741 - rmse: 5.2551 - val_loss: 1.1730 - val_accuracy: 0.5864 - val_rmse: 5.2501\n",
      "Epoch 210/400\n",
      "167/167 [==============================] - 153s 917ms/step - loss: 1.2076 - accuracy: 0.5760 - rmse: 5.2552 - val_loss: 1.1684 - val_accuracy: 0.5922 - val_rmse: 5.2500\n",
      "Epoch 211/400\n",
      "167/167 [==============================] - 151s 903ms/step - loss: 1.2029 - accuracy: 0.5754 - rmse: 5.2552 - val_loss: 1.1643 - val_accuracy: 0.5918 - val_rmse: 5.2501\n",
      "Epoch 212/400\n",
      "167/167 [==============================] - 151s 907ms/step - loss: 1.2020 - accuracy: 0.5774 - rmse: 5.2550 - val_loss: 1.1735 - val_accuracy: 0.5878 - val_rmse: 5.2501\n",
      "Epoch 213/400\n",
      "167/167 [==============================] - 150s 900ms/step - loss: 1.2014 - accuracy: 0.5784 - rmse: 5.2554 - val_loss: 1.1648 - val_accuracy: 0.5967 - val_rmse: 5.2500\n",
      "Epoch 214/400\n",
      "167/167 [==============================] - 150s 897ms/step - loss: 1.1982 - accuracy: 0.5768 - rmse: 5.2558 - val_loss: 1.1798 - val_accuracy: 0.5876 - val_rmse: 5.2500\n",
      "Epoch 215/400\n",
      "167/167 [==============================] - 153s 918ms/step - loss: 1.1973 - accuracy: 0.5807 - rmse: 5.2560 - val_loss: 1.1621 - val_accuracy: 0.5945 - val_rmse: 5.2500\n",
      "Epoch 216/400\n",
      "167/167 [==============================] - 151s 906ms/step - loss: 1.1969 - accuracy: 0.5806 - rmse: 5.2559 - val_loss: 1.1603 - val_accuracy: 0.5953 - val_rmse: 5.2501\n",
      "Epoch 217/400\n",
      "167/167 [==============================] - 151s 904ms/step - loss: 1.1922 - accuracy: 0.5782 - rmse: 5.2544 - val_loss: 1.1659 - val_accuracy: 0.5926 - val_rmse: 5.2502\n",
      "Epoch 218/400\n",
      "167/167 [==============================] - 152s 908ms/step - loss: 1.1928 - accuracy: 0.5830 - rmse: 5.2557 - val_loss: 1.1603 - val_accuracy: 0.5932 - val_rmse: 5.2502\n",
      "Epoch 219/400\n",
      "167/167 [==============================] - 151s 907ms/step - loss: 1.1932 - accuracy: 0.5804 - rmse: 5.2551 - val_loss: 1.1696 - val_accuracy: 0.5848 - val_rmse: 5.2502\n",
      "Epoch 220/400\n",
      "167/167 [==============================] - 151s 907ms/step - loss: 1.1892 - accuracy: 0.5819 - rmse: 5.2564 - val_loss: 1.1548 - val_accuracy: 0.5961 - val_rmse: 5.2501\n",
      "Epoch 221/400\n",
      "167/167 [==============================] - 151s 906ms/step - loss: 1.1830 - accuracy: 0.5847 - rmse: 5.2562 - val_loss: 1.2083 - val_accuracy: 0.5704 - val_rmse: 5.2503\n",
      "Epoch 222/400\n",
      "167/167 [==============================] - 162s 972ms/step - loss: 1.1847 - accuracy: 0.5856 - rmse: 5.2561 - val_loss: 1.1506 - val_accuracy: 0.5977 - val_rmse: 5.2501\n",
      "Epoch 223/400\n",
      "167/167 [==============================] - 158s 948ms/step - loss: 1.1813 - accuracy: 0.5868 - rmse: 5.2555 - val_loss: 1.1482 - val_accuracy: 0.6010 - val_rmse: 5.2501\n",
      "Epoch 224/400\n",
      "167/167 [==============================] - 156s 937ms/step - loss: 1.1784 - accuracy: 0.5882 - rmse: 5.2560 - val_loss: 1.1720 - val_accuracy: 0.5922 - val_rmse: 5.2501\n",
      "Epoch 225/400\n",
      "167/167 [==============================] - 156s 932ms/step - loss: 1.1803 - accuracy: 0.5845 - rmse: 5.2554 - val_loss: 1.1789 - val_accuracy: 0.5895 - val_rmse: 5.2499\n",
      "Epoch 226/400\n",
      "167/167 [==============================] - 154s 923ms/step - loss: 1.1806 - accuracy: 0.5832 - rmse: 5.2553 - val_loss: 1.1401 - val_accuracy: 0.5981 - val_rmse: 5.2503\n",
      "Epoch 227/400\n",
      "167/167 [==============================] - 154s 925ms/step - loss: 1.1792 - accuracy: 0.5858 - rmse: 5.2560 - val_loss: 1.1378 - val_accuracy: 0.6014 - val_rmse: 5.2502\n",
      "Epoch 228/400\n",
      "167/167 [==============================] - 156s 934ms/step - loss: 1.1727 - accuracy: 0.5889 - rmse: 5.2558 - val_loss: 1.1688 - val_accuracy: 0.5952 - val_rmse: 5.2501\n",
      "Epoch 229/400\n",
      "167/167 [==============================] - 154s 922ms/step - loss: 1.1738 - accuracy: 0.5858 - rmse: 5.2552 - val_loss: 1.1397 - val_accuracy: 0.5994 - val_rmse: 5.2502\n",
      "Epoch 230/400\n",
      "167/167 [==============================] - 153s 915ms/step - loss: 1.1728 - accuracy: 0.5910 - rmse: 5.2556 - val_loss: 1.1380 - val_accuracy: 0.6009 - val_rmse: 5.2503\n",
      "Epoch 231/400\n",
      "167/167 [==============================] - 154s 921ms/step - loss: 1.1741 - accuracy: 0.5860 - rmse: 5.2554 - val_loss: 1.1514 - val_accuracy: 0.5987 - val_rmse: 5.2500\n",
      "Epoch 232/400\n",
      "167/167 [==============================] - 153s 917ms/step - loss: 1.1695 - accuracy: 0.5905 - rmse: 5.2553 - val_loss: 1.1516 - val_accuracy: 0.5985 - val_rmse: 5.2500\n",
      "Epoch 233/400\n",
      "167/167 [==============================] - 151s 907ms/step - loss: 1.1683 - accuracy: 0.5911 - rmse: 5.2554 - val_loss: 1.1368 - val_accuracy: 0.5976 - val_rmse: 5.2504\n",
      "Epoch 234/400\n",
      "167/167 [==============================] - 152s 913ms/step - loss: 1.1664 - accuracy: 0.5887 - rmse: 5.2559 - val_loss: 1.1288 - val_accuracy: 0.6093 - val_rmse: 5.2502\n",
      "Epoch 235/400\n",
      "167/167 [==============================] - 153s 914ms/step - loss: 1.1612 - accuracy: 0.5941 - rmse: 5.2552 - val_loss: 1.1364 - val_accuracy: 0.6071 - val_rmse: 5.2502\n",
      "Epoch 236/400\n",
      "167/167 [==============================] - 152s 912ms/step - loss: 1.1600 - accuracy: 0.5922 - rmse: 5.2558 - val_loss: 1.1291 - val_accuracy: 0.6030 - val_rmse: 5.2503\n",
      "Epoch 237/400\n",
      "167/167 [==============================] - 152s 910ms/step - loss: 1.1568 - accuracy: 0.5932 - rmse: 5.2556 - val_loss: 1.1301 - val_accuracy: 0.6086 - val_rmse: 5.2502\n",
      "Epoch 238/400\n",
      "167/167 [==============================] - 152s 910ms/step - loss: 1.1559 - accuracy: 0.5939 - rmse: 5.2556 - val_loss: 1.1329 - val_accuracy: 0.6058 - val_rmse: 5.2502\n",
      "Epoch 239/400\n",
      "167/167 [==============================] - 153s 915ms/step - loss: 1.1547 - accuracy: 0.5946 - rmse: 5.2561 - val_loss: 1.1229 - val_accuracy: 0.6055 - val_rmse: 5.2504\n",
      "Epoch 240/400\n",
      "167/167 [==============================] - 152s 913ms/step - loss: 1.1594 - accuracy: 0.5929 - rmse: 5.2553 - val_loss: 1.1444 - val_accuracy: 0.5936 - val_rmse: 5.2504\n",
      "Epoch 241/400\n",
      "167/167 [==============================] - 154s 924ms/step - loss: 1.1606 - accuracy: 0.5953 - rmse: 5.2560 - val_loss: 1.1303 - val_accuracy: 0.6037 - val_rmse: 5.2503\n",
      "Epoch 242/400\n",
      "167/167 [==============================] - 154s 921ms/step - loss: 1.1528 - accuracy: 0.5961 - rmse: 5.2564 - val_loss: 1.1234 - val_accuracy: 0.6081 - val_rmse: 5.2502\n",
      "Epoch 243/400\n",
      "167/167 [==============================] - 152s 913ms/step - loss: 1.1521 - accuracy: 0.5971 - rmse: 5.2552 - val_loss: 1.1335 - val_accuracy: 0.6005 - val_rmse: 5.2504\n",
      "Epoch 244/400\n",
      "167/167 [==============================] - 151s 906ms/step - loss: 1.1460 - accuracy: 0.5948 - rmse: 5.2549 - val_loss: 1.1170 - val_accuracy: 0.6096 - val_rmse: 5.2503\n",
      "Epoch 245/400\n",
      "167/167 [==============================] - 162s 971ms/step - loss: 1.1459 - accuracy: 0.5991 - rmse: 5.2554 - val_loss: 1.1208 - val_accuracy: 0.6111 - val_rmse: 5.2502\n",
      "Epoch 246/400\n",
      "167/167 [==============================] - 158s 944ms/step - loss: 1.1427 - accuracy: 0.5988 - rmse: 5.2553 - val_loss: 1.1207 - val_accuracy: 0.6116 - val_rmse: 5.2503\n",
      "Epoch 247/400\n",
      "167/167 [==============================] - 152s 908ms/step - loss: 1.1423 - accuracy: 0.5990 - rmse: 5.2565 - val_loss: 1.1293 - val_accuracy: 0.6095 - val_rmse: 5.2502\n",
      "Epoch 248/400\n",
      "167/167 [==============================] - 152s 909ms/step - loss: 1.1413 - accuracy: 0.6006 - rmse: 5.2557 - val_loss: 1.1220 - val_accuracy: 0.6091 - val_rmse: 5.2502\n",
      "Epoch 249/400\n",
      "167/167 [==============================] - 151s 907ms/step - loss: 1.1387 - accuracy: 0.6012 - rmse: 5.2561 - val_loss: 1.1174 - val_accuracy: 0.6066 - val_rmse: 5.2504\n",
      "Epoch 250/400\n",
      "167/167 [==============================] - 151s 904ms/step - loss: 1.1377 - accuracy: 0.6000 - rmse: 5.2565 - val_loss: 1.1183 - val_accuracy: 0.6109 - val_rmse: 5.2504\n",
      "Epoch 251/400\n",
      "167/167 [==============================] - 151s 907ms/step - loss: 1.1374 - accuracy: 0.6006 - rmse: 5.2554 - val_loss: 1.1371 - val_accuracy: 0.5999 - val_rmse: 5.2505\n",
      "Epoch 252/400\n",
      "167/167 [==============================] - 151s 903ms/step - loss: 1.1379 - accuracy: 0.6025 - rmse: 5.2563 - val_loss: 1.1159 - val_accuracy: 0.6108 - val_rmse: 5.2503\n",
      "Epoch 253/400\n",
      "167/167 [==============================] - 151s 904ms/step - loss: 1.1306 - accuracy: 0.6033 - rmse: 5.2560 - val_loss: 1.1196 - val_accuracy: 0.6043 - val_rmse: 5.2504\n",
      "Epoch 254/400\n",
      "167/167 [==============================] - 152s 912ms/step - loss: 1.1310 - accuracy: 0.6022 - rmse: 5.2558 - val_loss: 1.1633 - val_accuracy: 0.5952 - val_rmse: 5.2502\n",
      "Epoch 255/400\n",
      "167/167 [==============================] - 153s 916ms/step - loss: 1.1309 - accuracy: 0.6048 - rmse: 5.2559 - val_loss: 1.1029 - val_accuracy: 0.6136 - val_rmse: 5.2505\n",
      "Epoch 256/400\n",
      "167/167 [==============================] - 152s 911ms/step - loss: 1.1288 - accuracy: 0.6058 - rmse: 5.2563 - val_loss: 1.1102 - val_accuracy: 0.6095 - val_rmse: 5.2504\n",
      "Epoch 257/400\n",
      "167/167 [==============================] - 152s 909ms/step - loss: 1.1234 - accuracy: 0.6069 - rmse: 5.2558 - val_loss: 1.1000 - val_accuracy: 0.6150 - val_rmse: 5.2504\n",
      "Epoch 258/400\n",
      "167/167 [==============================] - 152s 910ms/step - loss: 1.1223 - accuracy: 0.6081 - rmse: 5.2551 - val_loss: 1.1322 - val_accuracy: 0.6023 - val_rmse: 5.2505\n",
      "Epoch 259/400\n",
      "167/167 [==============================] - 151s 906ms/step - loss: 1.1219 - accuracy: 0.6058 - rmse: 5.2561 - val_loss: 1.1031 - val_accuracy: 0.6135 - val_rmse: 5.2504\n",
      "Epoch 260/400\n",
      "167/167 [==============================] - 153s 914ms/step - loss: 1.1172 - accuracy: 0.6071 - rmse: 5.2558 - val_loss: 1.1603 - val_accuracy: 0.6004 - val_rmse: 5.2502\n",
      "Epoch 261/400\n",
      "167/167 [==============================] - 153s 916ms/step - loss: 1.1185 - accuracy: 0.6070 - rmse: 5.2558 - val_loss: 1.0977 - val_accuracy: 0.6184 - val_rmse: 5.2504\n",
      "Epoch 262/400\n",
      "167/167 [==============================] - 153s 915ms/step - loss: 1.1113 - accuracy: 0.6115 - rmse: 5.2564 - val_loss: 1.1121 - val_accuracy: 0.6060 - val_rmse: 5.2506\n",
      "Epoch 263/400\n",
      "167/167 [==============================] - 150s 901ms/step - loss: 1.1137 - accuracy: 0.6106 - rmse: 5.2552 - val_loss: 1.0932 - val_accuracy: 0.6200 - val_rmse: 5.2504\n",
      "Epoch 264/400\n",
      "167/167 [==============================] - 153s 914ms/step - loss: 1.1162 - accuracy: 0.6072 - rmse: 5.2558 - val_loss: 1.0969 - val_accuracy: 0.6184 - val_rmse: 5.2504\n",
      "Epoch 265/400\n",
      "167/167 [==============================] - 152s 913ms/step - loss: 1.1098 - accuracy: 0.6094 - rmse: 5.2563 - val_loss: 1.1055 - val_accuracy: 0.6148 - val_rmse: 5.2504\n",
      "Epoch 266/400\n",
      "167/167 [==============================] - 153s 918ms/step - loss: 1.1085 - accuracy: 0.6116 - rmse: 5.2555 - val_loss: 1.0967 - val_accuracy: 0.6147 - val_rmse: 5.2504\n",
      "Epoch 267/400\n",
      "167/167 [==============================] - 151s 907ms/step - loss: 1.1080 - accuracy: 0.6122 - rmse: 5.2573 - val_loss: 1.1053 - val_accuracy: 0.6140 - val_rmse: 5.2506\n",
      "Epoch 268/400\n",
      "167/167 [==============================] - 152s 910ms/step - loss: 1.1067 - accuracy: 0.6126 - rmse: 5.2563 - val_loss: 1.1101 - val_accuracy: 0.6113 - val_rmse: 5.2503\n",
      "Epoch 269/400\n",
      "167/167 [==============================] - 153s 916ms/step - loss: 1.1030 - accuracy: 0.6130 - rmse: 5.2557 - val_loss: 1.1056 - val_accuracy: 0.6112 - val_rmse: 5.2505\n",
      "Epoch 270/400\n",
      "167/167 [==============================] - 158s 946ms/step - loss: 1.0987 - accuracy: 0.6154 - rmse: 5.2559 - val_loss: 1.0908 - val_accuracy: 0.6217 - val_rmse: 5.2505\n",
      "Epoch 271/400\n",
      "167/167 [==============================] - 148s 888ms/step - loss: 1.1029 - accuracy: 0.6149 - rmse: 5.2563 - val_loss: 1.0902 - val_accuracy: 0.6238 - val_rmse: 5.2504\n",
      "Epoch 272/400\n",
      "167/167 [==============================] - 153s 916ms/step - loss: 1.1017 - accuracy: 0.6122 - rmse: 5.2558 - val_loss: 1.0943 - val_accuracy: 0.6198 - val_rmse: 5.2504\n",
      "Epoch 273/400\n",
      "167/167 [==============================] - 152s 913ms/step - loss: 1.0982 - accuracy: 0.6155 - rmse: 5.2562 - val_loss: 1.0894 - val_accuracy: 0.6232 - val_rmse: 5.2504\n",
      "Epoch 274/400\n",
      "167/167 [==============================] - 151s 902ms/step - loss: 1.0947 - accuracy: 0.6181 - rmse: 5.2559 - val_loss: 1.0952 - val_accuracy: 0.6195 - val_rmse: 5.2504\n",
      "Epoch 275/400\n",
      "167/167 [==============================] - 152s 913ms/step - loss: 1.0937 - accuracy: 0.6179 - rmse: 5.2565 - val_loss: 1.0836 - val_accuracy: 0.6228 - val_rmse: 5.2505\n",
      "Epoch 276/400\n",
      "167/167 [==============================] - 151s 907ms/step - loss: 1.0927 - accuracy: 0.6166 - rmse: 5.2559 - val_loss: 1.0974 - val_accuracy: 0.6145 - val_rmse: 5.2506\n",
      "Epoch 277/400\n",
      "167/167 [==============================] - 152s 910ms/step - loss: 1.0915 - accuracy: 0.6170 - rmse: 5.2559 - val_loss: 1.0789 - val_accuracy: 0.6237 - val_rmse: 5.2505\n",
      "Epoch 278/400\n",
      "167/167 [==============================] - 153s 916ms/step - loss: 1.0850 - accuracy: 0.6183 - rmse: 5.2565 - val_loss: 1.0961 - val_accuracy: 0.6192 - val_rmse: 5.2505\n",
      "Epoch 279/400\n",
      "167/167 [==============================] - 153s 916ms/step - loss: 1.0881 - accuracy: 0.6208 - rmse: 5.2562 - val_loss: 1.0802 - val_accuracy: 0.6272 - val_rmse: 5.2505\n",
      "Epoch 280/400\n",
      "167/167 [==============================] - 152s 912ms/step - loss: 1.0871 - accuracy: 0.6202 - rmse: 5.2565 - val_loss: 1.0829 - val_accuracy: 0.6255 - val_rmse: 5.2505\n",
      "Epoch 281/400\n",
      "167/167 [==============================] - 152s 913ms/step - loss: 1.0814 - accuracy: 0.6199 - rmse: 5.2559 - val_loss: 1.0745 - val_accuracy: 0.6271 - val_rmse: 5.2505\n",
      "Epoch 282/400\n",
      "167/167 [==============================] - 152s 911ms/step - loss: 1.0823 - accuracy: 0.6226 - rmse: 5.2560 - val_loss: 1.0771 - val_accuracy: 0.6231 - val_rmse: 5.2507\n",
      "Epoch 283/400\n",
      "167/167 [==============================] - 154s 920ms/step - loss: 1.0825 - accuracy: 0.6206 - rmse: 5.2564 - val_loss: 1.0851 - val_accuracy: 0.6212 - val_rmse: 5.2507\n",
      "Epoch 284/400\n",
      "167/167 [==============================] - 153s 918ms/step - loss: 1.0801 - accuracy: 0.6225 - rmse: 5.2566 - val_loss: 1.0700 - val_accuracy: 0.6278 - val_rmse: 5.2506\n",
      "Epoch 285/400\n",
      "167/167 [==============================] - 153s 917ms/step - loss: 1.0791 - accuracy: 0.6221 - rmse: 5.2568 - val_loss: 1.1203 - val_accuracy: 0.6160 - val_rmse: 5.2506\n",
      "Epoch 286/400\n",
      "167/167 [==============================] - 151s 907ms/step - loss: 1.0769 - accuracy: 0.6228 - rmse: 5.2567 - val_loss: 1.0735 - val_accuracy: 0.6261 - val_rmse: 5.2507\n",
      "Epoch 287/400\n",
      "167/167 [==============================] - 152s 911ms/step - loss: 1.0744 - accuracy: 0.6237 - rmse: 5.2555 - val_loss: 1.0863 - val_accuracy: 0.6230 - val_rmse: 5.2504\n",
      "Epoch 288/400\n",
      "167/167 [==============================] - 154s 922ms/step - loss: 1.0747 - accuracy: 0.6250 - rmse: 5.2558 - val_loss: 1.0840 - val_accuracy: 0.6196 - val_rmse: 5.2506\n",
      "Epoch 289/400\n",
      "167/167 [==============================] - 153s 918ms/step - loss: 1.0732 - accuracy: 0.6269 - rmse: 5.2561 - val_loss: 1.0659 - val_accuracy: 0.6294 - val_rmse: 5.2506\n",
      "Epoch 290/400\n",
      "167/167 [==============================] - 153s 917ms/step - loss: 1.0680 - accuracy: 0.6276 - rmse: 5.2552 - val_loss: 1.0641 - val_accuracy: 0.6316 - val_rmse: 5.2506\n",
      "Epoch 291/400\n",
      "167/167 [==============================] - 152s 912ms/step - loss: 1.0663 - accuracy: 0.6277 - rmse: 5.2563 - val_loss: 1.0652 - val_accuracy: 0.6305 - val_rmse: 5.2506\n",
      "Epoch 292/400\n",
      "167/167 [==============================] - 153s 918ms/step - loss: 1.0642 - accuracy: 0.6290 - rmse: 5.2563 - val_loss: 1.0714 - val_accuracy: 0.6290 - val_rmse: 5.2507\n",
      "Epoch 293/400\n",
      "167/167 [==============================] - 153s 919ms/step - loss: 1.0627 - accuracy: 0.6268 - rmse: 5.2567 - val_loss: 1.0688 - val_accuracy: 0.6308 - val_rmse: 5.2505\n",
      "Epoch 294/400\n",
      "167/167 [==============================] - 153s 918ms/step - loss: 1.0606 - accuracy: 0.6300 - rmse: 5.2563 - val_loss: 1.0677 - val_accuracy: 0.6292 - val_rmse: 5.2506\n",
      "Epoch 295/400\n",
      "167/167 [==============================] - 153s 915ms/step - loss: 1.0625 - accuracy: 0.6289 - rmse: 5.2566 - val_loss: 1.0699 - val_accuracy: 0.6294 - val_rmse: 5.2506\n",
      "Epoch 296/400\n",
      "167/167 [==============================] - 153s 917ms/step - loss: 1.0640 - accuracy: 0.6267 - rmse: 5.2565 - val_loss: 1.0587 - val_accuracy: 0.6325 - val_rmse: 5.2506\n",
      "Epoch 297/400\n",
      "167/167 [==============================] - 153s 916ms/step - loss: 1.0574 - accuracy: 0.6294 - rmse: 5.2564 - val_loss: 1.0607 - val_accuracy: 0.6315 - val_rmse: 5.2506\n",
      "Epoch 298/400\n",
      "167/167 [==============================] - 154s 920ms/step - loss: 1.0598 - accuracy: 0.6287 - rmse: 5.2566 - val_loss: 1.0645 - val_accuracy: 0.6297 - val_rmse: 5.2507\n",
      "Epoch 299/400\n",
      "167/167 [==============================] - 153s 918ms/step - loss: 1.0592 - accuracy: 0.6291 - rmse: 5.2563 - val_loss: 1.0609 - val_accuracy: 0.6329 - val_rmse: 5.2507\n",
      "Epoch 300/400\n",
      "167/167 [==============================] - 153s 914ms/step - loss: 1.0567 - accuracy: 0.6319 - rmse: 5.2557 - val_loss: 1.0620 - val_accuracy: 0.6329 - val_rmse: 5.2506\n",
      "Epoch 301/400\n",
      "167/167 [==============================] - 153s 918ms/step - loss: 1.0509 - accuracy: 0.6325 - rmse: 5.2555 - val_loss: 1.0988 - val_accuracy: 0.6149 - val_rmse: 5.2509\n",
      "Epoch 302/400\n",
      "167/167 [==============================] - 154s 923ms/step - loss: 1.0544 - accuracy: 0.6319 - rmse: 5.2558 - val_loss: 1.0567 - val_accuracy: 0.6363 - val_rmse: 5.2506\n",
      "Epoch 303/400\n",
      "167/167 [==============================] - 154s 920ms/step - loss: 1.0507 - accuracy: 0.6350 - rmse: 5.2560 - val_loss: 1.0593 - val_accuracy: 0.6340 - val_rmse: 5.2506\n",
      "Epoch 304/400\n",
      "167/167 [==============================] - 153s 919ms/step - loss: 1.0465 - accuracy: 0.6340 - rmse: 5.2565 - val_loss: 1.0511 - val_accuracy: 0.6357 - val_rmse: 5.2507\n",
      "Epoch 305/400\n",
      "167/167 [==============================] - 153s 916ms/step - loss: 1.0515 - accuracy: 0.6344 - rmse: 5.2560 - val_loss: 1.0542 - val_accuracy: 0.6334 - val_rmse: 5.2506\n",
      "Epoch 306/400\n",
      "167/167 [==============================] - 153s 919ms/step - loss: 1.0414 - accuracy: 0.6375 - rmse: 5.2571 - val_loss: 1.0488 - val_accuracy: 0.6346 - val_rmse: 5.2508\n",
      "Epoch 307/400\n",
      "167/167 [==============================] - 153s 917ms/step - loss: 1.0469 - accuracy: 0.6327 - rmse: 5.2567 - val_loss: 1.0556 - val_accuracy: 0.6349 - val_rmse: 5.2507\n",
      "Epoch 308/400\n",
      "167/167 [==============================] - 153s 918ms/step - loss: 1.0389 - accuracy: 0.6370 - rmse: 5.2557 - val_loss: 1.0451 - val_accuracy: 0.6401 - val_rmse: 5.2507\n",
      "Epoch 309/400\n",
      "167/167 [==============================] - 153s 916ms/step - loss: 1.0429 - accuracy: 0.6339 - rmse: 5.2561 - val_loss: 1.0536 - val_accuracy: 0.6321 - val_rmse: 5.2508\n",
      "Epoch 310/400\n",
      "167/167 [==============================] - 155s 927ms/step - loss: 1.0390 - accuracy: 0.6393 - rmse: 5.2560 - val_loss: 1.0611 - val_accuracy: 0.6334 - val_rmse: 5.2506\n",
      "Epoch 311/400\n",
      "167/167 [==============================] - 153s 915ms/step - loss: 1.0339 - accuracy: 0.6372 - rmse: 5.2559 - val_loss: 1.0553 - val_accuracy: 0.6347 - val_rmse: 5.2508\n",
      "Epoch 312/400\n",
      "167/167 [==============================] - 156s 932ms/step - loss: 1.0360 - accuracy: 0.6381 - rmse: 5.2568 - val_loss: 1.0701 - val_accuracy: 0.6268 - val_rmse: 5.2507\n",
      "Epoch 313/400\n",
      "167/167 [==============================] - 154s 922ms/step - loss: 1.0365 - accuracy: 0.6382 - rmse: 5.2563 - val_loss: 1.0457 - val_accuracy: 0.6399 - val_rmse: 5.2507\n",
      "Epoch 314/400\n",
      "167/167 [==============================] - 153s 919ms/step - loss: 1.0363 - accuracy: 0.6388 - rmse: 5.2558 - val_loss: 1.0394 - val_accuracy: 0.6398 - val_rmse: 5.2508\n",
      "Epoch 315/400\n",
      "167/167 [==============================] - 153s 917ms/step - loss: 1.0319 - accuracy: 0.6396 - rmse: 5.2559 - val_loss: 1.0397 - val_accuracy: 0.6416 - val_rmse: 5.2507\n",
      "Epoch 316/400\n",
      "167/167 [==============================] - 155s 929ms/step - loss: 1.0276 - accuracy: 0.6416 - rmse: 5.2564 - val_loss: 1.0385 - val_accuracy: 0.6425 - val_rmse: 5.2507\n",
      "Epoch 317/400\n",
      "167/167 [==============================] - 154s 922ms/step - loss: 1.0301 - accuracy: 0.6407 - rmse: 5.2559 - val_loss: 1.0430 - val_accuracy: 0.6388 - val_rmse: 5.2508\n",
      "Epoch 318/400\n",
      "167/167 [==============================] - 154s 922ms/step - loss: 1.0288 - accuracy: 0.6409 - rmse: 5.2564 - val_loss: 1.0447 - val_accuracy: 0.6395 - val_rmse: 5.2507\n",
      "Epoch 319/400\n",
      "167/167 [==============================] - 153s 916ms/step - loss: 1.0258 - accuracy: 0.6403 - rmse: 5.2555 - val_loss: 1.0470 - val_accuracy: 0.6374 - val_rmse: 5.2508\n",
      "Epoch 320/400\n",
      "167/167 [==============================] - 155s 926ms/step - loss: 1.0241 - accuracy: 0.6425 - rmse: 5.2563 - val_loss: 1.0391 - val_accuracy: 0.6414 - val_rmse: 5.2507\n",
      "Epoch 321/400\n",
      "167/167 [==============================] - 154s 920ms/step - loss: 1.0223 - accuracy: 0.6427 - rmse: 5.2562 - val_loss: 1.0401 - val_accuracy: 0.6412 - val_rmse: 5.2506\n",
      "Epoch 322/400\n",
      "167/167 [==============================] - 154s 925ms/step - loss: 1.0207 - accuracy: 0.6415 - rmse: 5.2563 - val_loss: 1.0594 - val_accuracy: 0.6307 - val_rmse: 5.2509\n",
      "Epoch 323/400\n",
      "167/167 [==============================] - 154s 924ms/step - loss: 1.0205 - accuracy: 0.6428 - rmse: 5.2563 - val_loss: 1.0347 - val_accuracy: 0.6424 - val_rmse: 5.2507\n",
      "Epoch 324/400\n",
      "167/167 [==============================] - 154s 921ms/step - loss: 1.0206 - accuracy: 0.6431 - rmse: 5.2566 - val_loss: 1.0362 - val_accuracy: 0.6404 - val_rmse: 5.2508\n",
      "Epoch 325/400\n",
      "167/167 [==============================] - 154s 921ms/step - loss: 1.0185 - accuracy: 0.6437 - rmse: 5.2564 - val_loss: 1.0726 - val_accuracy: 0.6239 - val_rmse: 5.2506\n",
      "Epoch 326/400\n",
      "167/167 [==============================] - 155s 928ms/step - loss: 1.0141 - accuracy: 0.6448 - rmse: 5.2569 - val_loss: 1.0510 - val_accuracy: 0.6321 - val_rmse: 5.2509\n",
      "Epoch 327/400\n",
      "167/167 [==============================] - 153s 917ms/step - loss: 1.0145 - accuracy: 0.6453 - rmse: 5.2566 - val_loss: 1.0294 - val_accuracy: 0.6437 - val_rmse: 5.2507\n",
      "Epoch 328/400\n",
      "167/167 [==============================] - 153s 919ms/step - loss: 1.0143 - accuracy: 0.6449 - rmse: 5.2564 - val_loss: 1.0442 - val_accuracy: 0.6416 - val_rmse: 5.2508\n",
      "Epoch 329/400\n",
      "167/167 [==============================] - 154s 923ms/step - loss: 1.0093 - accuracy: 0.6469 - rmse: 5.2566 - val_loss: 1.0429 - val_accuracy: 0.6402 - val_rmse: 5.2508\n",
      "Epoch 330/400\n",
      "167/167 [==============================] - 153s 918ms/step - loss: 1.0050 - accuracy: 0.6490 - rmse: 5.2558 - val_loss: 1.0293 - val_accuracy: 0.6437 - val_rmse: 5.2508\n",
      "Epoch 331/400\n",
      "167/167 [==============================] - 153s 919ms/step - loss: 1.0077 - accuracy: 0.6467 - rmse: 5.2568 - val_loss: 1.0336 - val_accuracy: 0.6414 - val_rmse: 5.2508\n",
      "Epoch 332/400\n",
      "167/167 [==============================] - 153s 919ms/step - loss: 0.9996 - accuracy: 0.6511 - rmse: 5.2561 - val_loss: 1.0267 - val_accuracy: 0.6443 - val_rmse: 5.2508\n",
      "Epoch 333/400\n",
      "167/167 [==============================] - 154s 925ms/step - loss: 1.0004 - accuracy: 0.6511 - rmse: 5.2562 - val_loss: 1.0361 - val_accuracy: 0.6380 - val_rmse: 5.2509\n",
      "Epoch 334/400\n",
      "167/167 [==============================] - 153s 919ms/step - loss: 1.0042 - accuracy: 0.6507 - rmse: 5.2566 - val_loss: 1.0210 - val_accuracy: 0.6478 - val_rmse: 5.2508\n",
      "Epoch 335/400\n",
      "167/167 [==============================] - 156s 936ms/step - loss: 1.0014 - accuracy: 0.6497 - rmse: 5.2553 - val_loss: 1.0371 - val_accuracy: 0.6424 - val_rmse: 5.2508\n",
      "Epoch 336/400\n",
      "167/167 [==============================] - 154s 925ms/step - loss: 0.9998 - accuracy: 0.6519 - rmse: 5.2561 - val_loss: 1.0202 - val_accuracy: 0.6477 - val_rmse: 5.2507\n",
      "Epoch 337/400\n",
      "167/167 [==============================] - 154s 920ms/step - loss: 0.9965 - accuracy: 0.6526 - rmse: 5.2562 - val_loss: 1.0317 - val_accuracy: 0.6433 - val_rmse: 5.2510\n",
      "Epoch 338/400\n",
      "167/167 [==============================] - 154s 920ms/step - loss: 0.9977 - accuracy: 0.6514 - rmse: 5.2565 - val_loss: 1.0282 - val_accuracy: 0.6439 - val_rmse: 5.2509\n",
      "Epoch 339/400\n",
      "167/167 [==============================] - 154s 920ms/step - loss: 0.9914 - accuracy: 0.6535 - rmse: 5.2564 - val_loss: 1.0211 - val_accuracy: 0.6482 - val_rmse: 5.2509\n",
      "Epoch 340/400\n",
      "167/167 [==============================] - 155s 927ms/step - loss: 0.9937 - accuracy: 0.6537 - rmse: 5.2563 - val_loss: 1.0170 - val_accuracy: 0.6481 - val_rmse: 5.2509\n",
      "Epoch 341/400\n",
      "167/167 [==============================] - 154s 925ms/step - loss: 0.9888 - accuracy: 0.6549 - rmse: 5.2566 - val_loss: 1.0188 - val_accuracy: 0.6453 - val_rmse: 5.2509\n",
      "Epoch 342/400\n",
      "167/167 [==============================] - 153s 914ms/step - loss: 0.9900 - accuracy: 0.6557 - rmse: 5.2561 - val_loss: 1.0157 - val_accuracy: 0.6511 - val_rmse: 5.2508\n",
      "Epoch 343/400\n",
      "167/167 [==============================] - 153s 918ms/step - loss: 0.9843 - accuracy: 0.6565 - rmse: 5.2566 - val_loss: 1.0350 - val_accuracy: 0.6409 - val_rmse: 5.2510\n",
      "Epoch 344/400\n",
      "167/167 [==============================] - 154s 923ms/step - loss: 0.9859 - accuracy: 0.6561 - rmse: 5.2565 - val_loss: 1.0143 - val_accuracy: 0.6484 - val_rmse: 5.2509\n",
      "Epoch 345/400\n",
      "167/167 [==============================] - 154s 920ms/step - loss: 0.9844 - accuracy: 0.6571 - rmse: 5.2561 - val_loss: 1.0288 - val_accuracy: 0.6441 - val_rmse: 5.2510\n",
      "Epoch 346/400\n",
      "167/167 [==============================] - 153s 918ms/step - loss: 0.9828 - accuracy: 0.6573 - rmse: 5.2554 - val_loss: 1.0175 - val_accuracy: 0.6493 - val_rmse: 5.2509\n",
      "Epoch 347/400\n",
      "167/167 [==============================] - 153s 918ms/step - loss: 0.9816 - accuracy: 0.6581 - rmse: 5.2565 - val_loss: 1.0180 - val_accuracy: 0.6457 - val_rmse: 5.2510\n",
      "Epoch 348/400\n",
      "167/167 [==============================] - 152s 911ms/step - loss: 0.9805 - accuracy: 0.6579 - rmse: 5.2568 - val_loss: 1.0205 - val_accuracy: 0.6496 - val_rmse: 5.2509\n",
      "Epoch 349/400\n",
      "167/167 [==============================] - 154s 923ms/step - loss: 0.9793 - accuracy: 0.6597 - rmse: 5.2568 - val_loss: 1.0131 - val_accuracy: 0.6474 - val_rmse: 5.2509\n",
      "Epoch 350/400\n",
      "167/167 [==============================] - 154s 922ms/step - loss: 0.9767 - accuracy: 0.6622 - rmse: 5.2567 - val_loss: 1.0173 - val_accuracy: 0.6469 - val_rmse: 5.2509\n",
      "Epoch 351/400\n",
      "167/167 [==============================] - 154s 920ms/step - loss: 0.9783 - accuracy: 0.6578 - rmse: 5.2560 - val_loss: 1.0218 - val_accuracy: 0.6494 - val_rmse: 5.2510\n",
      "Epoch 352/400\n",
      "167/167 [==============================] - 153s 917ms/step - loss: 0.9774 - accuracy: 0.6612 - rmse: 5.2570 - val_loss: 1.0126 - val_accuracy: 0.6495 - val_rmse: 5.2509\n",
      "Epoch 353/400\n",
      "167/167 [==============================] - 153s 917ms/step - loss: 0.9745 - accuracy: 0.6583 - rmse: 5.2556 - val_loss: 1.0057 - val_accuracy: 0.6526 - val_rmse: 5.2510\n",
      "Epoch 354/400\n",
      "167/167 [==============================] - 154s 922ms/step - loss: 0.9730 - accuracy: 0.6595 - rmse: 5.2569 - val_loss: 1.0117 - val_accuracy: 0.6474 - val_rmse: 5.2510\n",
      "Epoch 355/400\n",
      "167/167 [==============================] - 154s 922ms/step - loss: 0.9708 - accuracy: 0.6608 - rmse: 5.2561 - val_loss: 1.0069 - val_accuracy: 0.6515 - val_rmse: 5.2509\n",
      "Epoch 356/400\n",
      "167/167 [==============================] - 155s 928ms/step - loss: 0.9711 - accuracy: 0.6608 - rmse: 5.2565 - val_loss: 1.0127 - val_accuracy: 0.6487 - val_rmse: 5.2509\n",
      "Epoch 357/400\n",
      "167/167 [==============================] - 154s 925ms/step - loss: 0.9649 - accuracy: 0.6638 - rmse: 5.2558 - val_loss: 1.0137 - val_accuracy: 0.6519 - val_rmse: 5.2509\n",
      "Epoch 358/400\n",
      "167/167 [==============================] - 154s 922ms/step - loss: 0.9641 - accuracy: 0.6635 - rmse: 5.2564 - val_loss: 1.0058 - val_accuracy: 0.6513 - val_rmse: 5.2510\n",
      "Epoch 359/400\n",
      "167/167 [==============================] - 158s 944ms/step - loss: 0.9650 - accuracy: 0.6633 - rmse: 5.2563 - val_loss: 1.0205 - val_accuracy: 0.6463 - val_rmse: 5.2510\n",
      "Epoch 360/400\n",
      "167/167 [==============================] - 158s 944ms/step - loss: 0.9672 - accuracy: 0.6617 - rmse: 5.2565 - val_loss: 1.0265 - val_accuracy: 0.6443 - val_rmse: 5.2508\n",
      "Epoch 361/400\n",
      "167/167 [==============================] - 156s 932ms/step - loss: 0.9592 - accuracy: 0.6640 - rmse: 5.2557 - val_loss: 1.0026 - val_accuracy: 0.6566 - val_rmse: 5.2510\n",
      "Epoch 362/400\n",
      "167/167 [==============================] - 155s 927ms/step - loss: 0.9626 - accuracy: 0.6654 - rmse: 5.2564 - val_loss: 1.0015 - val_accuracy: 0.6552 - val_rmse: 5.2510\n",
      "Epoch 363/400\n",
      "167/167 [==============================] - 156s 934ms/step - loss: 0.9600 - accuracy: 0.6661 - rmse: 5.2567 - val_loss: 1.0099 - val_accuracy: 0.6516 - val_rmse: 5.2509\n",
      "Epoch 364/400\n",
      "167/167 [==============================] - 156s 935ms/step - loss: 0.9595 - accuracy: 0.6638 - rmse: 5.2573 - val_loss: 1.0069 - val_accuracy: 0.6535 - val_rmse: 5.2510\n",
      "Epoch 365/400\n",
      "167/167 [==============================] - 154s 925ms/step - loss: 0.9581 - accuracy: 0.6634 - rmse: 5.2564 - val_loss: 0.9995 - val_accuracy: 0.6552 - val_rmse: 5.2510\n",
      "Epoch 366/400\n",
      "167/167 [==============================] - 155s 931ms/step - loss: 0.9533 - accuracy: 0.6691 - rmse: 5.2562 - val_loss: 1.0040 - val_accuracy: 0.6547 - val_rmse: 5.2510\n",
      "Epoch 367/400\n",
      "167/167 [==============================] - 157s 937ms/step - loss: 0.9518 - accuracy: 0.6669 - rmse: 5.2564 - val_loss: 1.0000 - val_accuracy: 0.6532 - val_rmse: 5.2511\n",
      "Epoch 368/400\n",
      "167/167 [==============================] - 156s 933ms/step - loss: 0.9549 - accuracy: 0.6659 - rmse: 5.2563 - val_loss: 1.0246 - val_accuracy: 0.6426 - val_rmse: 5.2510\n",
      "Epoch 369/400\n",
      "167/167 [==============================] - 154s 923ms/step - loss: 0.9481 - accuracy: 0.6700 - rmse: 5.2567 - val_loss: 1.0019 - val_accuracy: 0.6549 - val_rmse: 5.2511\n",
      "Epoch 370/400\n",
      "167/167 [==============================] - 154s 922ms/step - loss: 0.9486 - accuracy: 0.6699 - rmse: 5.2568 - val_loss: 1.0155 - val_accuracy: 0.6484 - val_rmse: 5.2510\n",
      "Epoch 371/400\n",
      "167/167 [==============================] - 154s 922ms/step - loss: 0.9502 - accuracy: 0.6672 - rmse: 5.2567 - val_loss: 0.9957 - val_accuracy: 0.6573 - val_rmse: 5.2511\n",
      "Epoch 372/400\n",
      "167/167 [==============================] - 154s 922ms/step - loss: 0.9499 - accuracy: 0.6680 - rmse: 5.2563 - val_loss: 1.0121 - val_accuracy: 0.6470 - val_rmse: 5.2510\n",
      "Epoch 373/400\n",
      "167/167 [==============================] - 154s 925ms/step - loss: 0.9412 - accuracy: 0.6703 - rmse: 5.2572 - val_loss: 0.9932 - val_accuracy: 0.6592 - val_rmse: 5.2511\n",
      "Epoch 374/400\n",
      "167/167 [==============================] - 155s 926ms/step - loss: 0.9389 - accuracy: 0.6715 - rmse: 5.2573 - val_loss: 1.0044 - val_accuracy: 0.6505 - val_rmse: 5.2510\n",
      "Epoch 375/400\n",
      "167/167 [==============================] - 157s 941ms/step - loss: 0.9412 - accuracy: 0.6719 - rmse: 5.2567 - val_loss: 0.9966 - val_accuracy: 0.6563 - val_rmse: 5.2510\n",
      "Epoch 376/400\n",
      "167/167 [==============================] - 156s 934ms/step - loss: 0.9391 - accuracy: 0.6721 - rmse: 5.2564 - val_loss: 1.0164 - val_accuracy: 0.6477 - val_rmse: 5.2511\n",
      "Epoch 377/400\n",
      "167/167 [==============================] - 155s 927ms/step - loss: 0.9370 - accuracy: 0.6721 - rmse: 5.2566 - val_loss: 1.0095 - val_accuracy: 0.6486 - val_rmse: 5.2510\n",
      "Epoch 378/400\n",
      "167/167 [==============================] - 155s 931ms/step - loss: 0.9367 - accuracy: 0.6735 - rmse: 5.2561 - val_loss: 0.9916 - val_accuracy: 0.6549 - val_rmse: 5.2512\n",
      "Epoch 379/400\n",
      "167/167 [==============================] - 154s 925ms/step - loss: 0.9389 - accuracy: 0.6748 - rmse: 5.2566 - val_loss: 1.0036 - val_accuracy: 0.6511 - val_rmse: 5.2511\n",
      "Epoch 380/400\n",
      "167/167 [==============================] - 154s 923ms/step - loss: 0.9299 - accuracy: 0.6756 - rmse: 5.2560 - val_loss: 1.0083 - val_accuracy: 0.6506 - val_rmse: 5.2511\n",
      "Epoch 381/400\n",
      "167/167 [==============================] - 153s 919ms/step - loss: 0.9361 - accuracy: 0.6753 - rmse: 5.2567 - val_loss: 0.9986 - val_accuracy: 0.6558 - val_rmse: 5.2511\n",
      "Epoch 382/400\n",
      "167/167 [==============================] - 156s 934ms/step - loss: 0.9284 - accuracy: 0.6758 - rmse: 5.2566 - val_loss: 1.0062 - val_accuracy: 0.6505 - val_rmse: 5.2512\n",
      "Epoch 383/400\n",
      "167/167 [==============================] - 154s 924ms/step - loss: 0.9312 - accuracy: 0.6735 - rmse: 5.2563 - val_loss: 0.9861 - val_accuracy: 0.6595 - val_rmse: 5.2511\n",
      "Epoch 384/400\n",
      "167/167 [==============================] - 154s 924ms/step - loss: 0.9304 - accuracy: 0.6734 - rmse: 5.2560 - val_loss: 1.0048 - val_accuracy: 0.6514 - val_rmse: 5.2511\n",
      "Epoch 385/400\n",
      "167/167 [==============================] - 154s 920ms/step - loss: 0.9260 - accuracy: 0.6753 - rmse: 5.2569 - val_loss: 0.9912 - val_accuracy: 0.6584 - val_rmse: 5.2512\n",
      "Epoch 386/400\n",
      "167/167 [==============================] - 154s 920ms/step - loss: 0.9269 - accuracy: 0.6763 - rmse: 5.2565 - val_loss: 0.9932 - val_accuracy: 0.6577 - val_rmse: 5.2510\n",
      "Epoch 387/400\n",
      "167/167 [==============================] - 156s 934ms/step - loss: 0.9255 - accuracy: 0.6781 - rmse: 5.2563 - val_loss: 1.0112 - val_accuracy: 0.6496 - val_rmse: 5.2513\n",
      "Epoch 388/400\n",
      "167/167 [==============================] - 154s 923ms/step - loss: 0.9208 - accuracy: 0.6799 - rmse: 5.2574 - val_loss: 0.9963 - val_accuracy: 0.6533 - val_rmse: 5.2513\n",
      "Epoch 389/400\n",
      "167/167 [==============================] - 153s 918ms/step - loss: 0.9233 - accuracy: 0.6771 - rmse: 5.2556 - val_loss: 0.9940 - val_accuracy: 0.6532 - val_rmse: 5.2511\n",
      "Epoch 390/400\n",
      "167/167 [==============================] - 159s 953ms/step - loss: 0.9173 - accuracy: 0.6793 - rmse: 5.2564 - val_loss: 0.9830 - val_accuracy: 0.6578 - val_rmse: 5.2511\n",
      "Epoch 391/400\n",
      "167/167 [==============================] - 160s 958ms/step - loss: 0.9159 - accuracy: 0.6803 - rmse: 5.2560 - val_loss: 0.9930 - val_accuracy: 0.6558 - val_rmse: 5.2511\n",
      "Epoch 392/400\n",
      "167/167 [==============================] - 164s 985ms/step - loss: 0.9167 - accuracy: 0.6795 - rmse: 5.2562 - val_loss: 0.9937 - val_accuracy: 0.6568 - val_rmse: 5.2511\n",
      "Epoch 393/400\n",
      "167/167 [==============================] - 156s 936ms/step - loss: 0.9130 - accuracy: 0.6815 - rmse: 5.2566 - val_loss: 0.9833 - val_accuracy: 0.6599 - val_rmse: 5.2512\n",
      "Epoch 394/400\n",
      "167/167 [==============================] - 156s 937ms/step - loss: 0.9112 - accuracy: 0.6805 - rmse: 5.2561 - val_loss: 0.9903 - val_accuracy: 0.6560 - val_rmse: 5.2512\n",
      "Epoch 395/400\n",
      "167/167 [==============================] - 158s 945ms/step - loss: 0.9097 - accuracy: 0.6826 - rmse: 5.2569 - val_loss: 0.9892 - val_accuracy: 0.6577 - val_rmse: 5.2512\n",
      "Epoch 396/400\n",
      "167/167 [==============================] - 162s 968ms/step - loss: 0.9073 - accuracy: 0.6834 - rmse: 5.2560 - val_loss: 0.9849 - val_accuracy: 0.6601 - val_rmse: 5.2512\n",
      "Epoch 397/400\n",
      "167/167 [==============================] - 158s 948ms/step - loss: 0.9116 - accuracy: 0.6816 - rmse: 5.2573 - val_loss: 0.9780 - val_accuracy: 0.6599 - val_rmse: 5.2512\n",
      "Epoch 398/400\n",
      "167/167 [==============================] - 160s 957ms/step - loss: 0.9087 - accuracy: 0.6838 - rmse: 5.2567 - val_loss: 0.9808 - val_accuracy: 0.6623 - val_rmse: 5.2512\n",
      "Epoch 399/400\n",
      "167/167 [==============================] - 173s 1s/step - loss: 0.9040 - accuracy: 0.6829 - rmse: 5.2568 - val_loss: 0.9767 - val_accuracy: 0.6632 - val_rmse: 5.2512\n",
      "Epoch 400/400\n",
      "167/167 [==============================] - 167s 998ms/step - loss: 0.9050 - accuracy: 0.6823 - rmse: 5.2572 - val_loss: 1.0062 - val_accuracy: 0.6521 - val_rmse: 5.2513\n"
     ]
    }
   ],
   "source": [
    "# Train the model on 50k training images and validate on 10k test images\n",
    "# Train the model with differential privacy\n",
    "history = model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "690e5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, multiply accuracy by 100 to convert to percentage\n",
    "final_accuracy = history.history['accuracy'][-1] * 100\n",
    "final_val_accuracy = history.history['val_accuracy'][-1] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a0cea5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training accuracy: 68.23199987411499%\n",
      "Final validation accuracy: 65.21000266075134%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final training accuracy: {final_accuracy}%\")\n",
    "print(f\"Final validation accuracy: {final_val_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e781724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD with sampling rate = 0.6% and noise_multiplier = 2.5 iterated over 66667 steps satisfies differential privacy with eps = 3.32 and delta = 1e-05.\n",
      "The optimal RDP order is 8.0.\n",
      "Trained with DP-SGD with  = 3.32 and  = 8.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the privacy budget expended during training\n",
    "epsilon, delta = compute_dp_sgd_privacy.compute_dp_sgd_privacy(\n",
    "    len(train_data), batch_size, noise_multiplier, epochs, delta=1e-5)\n",
    "print(f\"Trained with DP-SGD with  = {epsilon:.2f} and  = {delta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8863c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def compute_dp_adam_privacy(train_data_length, batch_size, noise_multiplier, epochs, delta=1e-5):\n",
    "    # Placeholder for privacy calculation logic\n",
    "    # The actual implementation would depend on the differential privacy library being used.\n",
    "    # For example, TensorFl   ow Privacy provides tools to compute the privacy budget (epsilon).\n",
    "\n",
    "    # The function below is conceptual and does not represent actual code from TensorFlow Privacy or similar libraries.\n",
    "    # You will need to replace this with the appropriate function call from the differential privacy library you are using.\n",
    "    #epsilon = some_dp_library.compute_epsilon(train_data_length, batch_size, noise_multiplier, epochs, delta)\n",
    "\n",
    "    #return epsilon, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132003c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the privacy budget expended during training\n",
    "#epsilon, delta = compute_dp_adam_privacy(len(train_data), batch_size, noise_multiplier, epochs)\n",
    "#print(\"Epsilon:\", epsilon, \"Delta:\", delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398d3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc3596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
